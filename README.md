Playing around with diffusion, clip, and RL.

Exploration-phase of 6.7960 final project.

## Setup
- Install requirements using command in requirements.txt
  - Ensure you're using `uv` with a `venv`
- Download dataset from instructions in dataset/README.md
- Configure accelerate with `accelerate config` - default options are fine
  - If using multi-gpu training, make sure to select the # gpus
- Launch RL DDPO loop with `accelerate launch rl_trainer.py`

During training or at the end, upload checkpoints with: 
```bash
aws s3 cp ddpo_logs/{TIMESTAMP}/checkpoints s3://hectorastrom-dl-final/checkpoints/{WANDB_RUN_NAME}/{optional: epoch_num} --recursive
```

## File structure
**Relevant files to RL Objective**:
- COD_dataset.py - builds the dataset object to work with the COD dataset
- ddpo.py        - modifies and extends the (deprecated) DDPOTrainer class from
  Huggingface TRL to support image inputs, and run RL from the 10 denoising
  steps back we go from that image
- reward.py      - defines the reward function (CLIP accuracy)
- rl_trainer.py  - runs the RL loop, combining our custom pipeline and
  ImageDDPOTrainer class from ddpo.py

**Other files (experimentation)**:
- text2img.py - uses SD1.5 to generate images from a text prompt
- img2img.py - uses SD1.5 to modify any image -- usually the one generated by
  text2img.py -- according to a text prompt
- clip.py - uses CLIP to provide confidence scores for a set of candidate labels
  corresponding to an image
- lcm.py - Uses a latent consistency model -- a version of a diffusion model
  that can generate in 2-4 steps (compared to 50+ normally) by using a closed
  from ODE solution to denoising -- to generate an image from text
- clip_classifier.py - early work to get dataset loading working & use that for
  clip score baselining
- clip_visualizer.py - image visualization from dataset to ensure our dataset
  object works

