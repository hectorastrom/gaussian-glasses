# Diffusion Lens
Making hidden objects classifier-legible through diffusion.

## Setup
- Ensure you're using `uv` with a `venv`
- Install requirements from `pyproject.toml` using `uv sync`
- Download dataset from instructions in `data/README.md`
- Configure accelerate with `accelerate config` — default options are fine  
  - If using multi-GPU training, make sure to select the number of GPUs
- Launch RL DDPO loop with:
  ```bash
  accelerate launch -m rl.rl_trainer
  ```

### Usage Options

**Modes**
You can run the trainer in two modes using the `--mode` argument:
- `rl` (default): Runs the full Reinforcement Learning loop with diffusion.
- `eval_classifier`: Evaluates the CLIP classifier accuracy on the dataset without diffusion.

**Datasets**
You can choose the dataset using the `--dataset` argument:
- `cod` (default): Use the COD10K camouflage dataset.
- `cifar10-c`: Use the CIFAR-10-C corruption dataset. Requires `--corruption` and `--severity` arguments.
- `tiny-imagenet-c`: Use the Tiny-ImageNet-C corruption dataset. Requires `--corruption` and `--severity` arguments.
- `imagenet-c`: Use the ImageNet-C corruption dataset. Requires `--corruption` and `--severity` arguments.

> **Note**: `eval_classifier` mode supports `cifar10-c`, `tiny-imagenet-c`, and `imagenet-c`, but not `cod`.

During training or at the end, upload checkpoints with:

```bash
aws s3 cp ddpo_logs/{TIMESTAMP}/checkpoints s3://hectorastrom-dl-final/checkpoints/{WANDB_RUN_NAME}/{optional: epoch_num} --recursive
```

## File structure

**Relevant files to RL Objective**:

* `data/COD_Dataset.py` — builds the dataset object to work with the COD dataset
* `rl/ddpo.py` — modifies and extends the (deprecated) DDPOTrainer class from HuggingFace TRL to support image inputs and run RL from the 10 denoising steps back we go from that image
* `rl/reward.py` — defines the reward function (CLIP accuracy)
* `rl/rl_trainer.py` — runs the RL loop, combining our custom pipeline and the
  `ImageDDPOTrainer` class from `ddpo.py`

**Benchmarking files**
* `exploration/clip_classifier.py` (Hector) - o.g. used for CLIP only
* `clip_eval.py` (Kevin) - used for SFT vs. CLIP
* `rl/eval_rl.py` (Hector) - used for RL vs. CLIP

**Other files (experimentation)**:

* `exploration/text2img.py` — uses SD1.5 to generate images from a text prompt
* `exploration/img2img.py` — uses SD1.5 to modify any image — usually the one generated by `text2img.py` — according to a text prompt
* `exploration/clip.py` — uses CLIP to provide confidence scores for a set of candidate labels corresponding to an image
* `exploration/lcm.py` — uses a latent consistency model to generate an image from text in 2–4 steps
* `exploration/clip_classifier.py` — early work to get dataset loading working and use that for CLIP score baselining

## Dev Notes

* After `toTensor()` in `build_COD_torch_dataset`, the previously PIL images become tensors `(C, H, W)` with pixel values `[0, 1]`

  * In the val hook, we explicitly divide by `255.0` because we're manually converting from PIL (the pipeline output) to tensor. You can avoid this by setting `pipeline.output_type='pt'`, but here we needed PIL images to render for wandb.
