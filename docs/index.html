<!DOCTYPE html>
<html>

<head>
	<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
	<!-- Add MathJax script for equation rendering -->
	<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

	<link rel="shortcut icon" href="images/icon.ico" />
	<style type="text/css">
		@font-face {
			font-family: 'Iowan Old Style';
			src: url('fonts/iowanoldst.ttf') format('truetype');
			font-weight: normal;
			font-style: normal;
		}

		@font-face {
			font-family: 'Iowan Old Style';
			src: url('fonts/iowanoldst-italic.ttf') format('truetype');
			font-weight: normal;
			font-style: italic;
		}

		@font-face {
			font-family: 'Iowan Old Style';
			src: url('fonts/iowanoldst-bold-italic.ttf') format('truetype');
			font-weight: bold;
			font-style: italic;
		}

		@font-face {
			font-family: 'Caudex-Bold';
			src: url('fonts/Caudex-Bold.ttf') format('truetype');
			font-weight: bold;
			font-style: normal;
		}

		@font-face {
			font-family: 'Iowan Old Style Heavy Bold';
			src: url('fonts/iowanoldst-heavy-bold.ttf') format('truetype');
			font-weight: 900;
			font-style: normal;
		}

		.extrabold {
			font-family: 'Iowan Old Style Heavy Bold', serif;
			font-weight: 900;
		}

		.heavy-caption {
			font-size: 1.1em;
			font-weight: bold;
			margin-top: 15px;
			margin-bottom: 30px;
			color: black;
			text-align: center;
		}

		body {
			background-color: #f5f9ff;
		}

		/* Hide both math displays initially, will display based on JS detection */
		.mathjax-mobile,
		.mathml-non-mobile {
			display: none;
		}

		/* Show the MathML content by default on non-mobile devices */
		.show-mathml .mathml-non-mobile {
			display: block;
		}

		.show-mathjax .mathjax-mobile {
			display: block;
		}

		.content-margin-container {
			display: flex;
			width: 100%;
			/* Ensure the container is full width */
			justify-content: left;
			/* Horizontally centers the children in the container */
			align-items: stretch;
			/* Vertically centers the children in the container */
		}

		.main-content-block {
			width: 60%;
			/* Change this percentage as needed */
			max-width: 1100px;
			/* Optional: Maximum width */
			background-color: #fff;
			border-left: 1px solid #DDD;
			border-right: 1px solid #DDD;
			padding: 8px 8px 8px 8px;
			font-family: 'Iowan Old Style', serif;
		}

		.margin-left-block {
			font-size: 14px;
			width: 25%;
			/* Change this percentage as needed */
			max-width: 195px;
			/* Optional: Maximum width */
			position: relative;
			margin-left: 10px;
			text-align: left;
			font-family: 'Iowan Old Style', serif;
			padding: 5px 5px 5px 15px;
		}

		.margin-right-block {
			font-family: 'Iowan Old Style', serif;
			font-size: 14px;
			width: 25%;
			/* Change this percentage as needed */
			max-width: 256px;
			/* Optional: Maximum width */
			position: relative;
			text-align: left; 
			padding-left: 10px;
			padding-right: 5px;
			/* top right bottom left */
		}

		#outline-nav {
			position: fixed;
			top: max(20%, 120px);
			left: 10px;
			z-index: 999;
			pointer-events: auto;
			max-width: inherit;
		}

		.citation-margin {
			width: 100%;
			font-size: 12px;
			color: #555;
			padding: 5px 0 5px 10px;
			box-sizing: border-box;
			margin-bottom: 8px; /* Space between stacked citations */
		}

		.citation-margin a {
			color: #555;
			text-decoration: underline;
		}

		.citation-margin a:hover {
			color: #000;
		}

		/* Hide the bottom references section but keep it for data */
		#citations {
			display: none;
		}

		img {
			max-width: 100%;
			/* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
		}

		.my-video {
			max-width: 100%;
			/* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
		}

		/* Hide both video displays initially, will display based on JS detection */
		.vid-mobile,
		.vid-non-mobile {
			display: none;
		}

		/* Show the video content by default on non-mobile devices */
		.show-vid-mobile .vid-mobile {
			display: block;
		}

		.show-vid-non-mobile .vid-non-mobile {
			display: block;
		}

		a:link,
		a:visited {
			color: #0e7862;
			/*#1367a7;*/
			text-decoration: none;
		}

		a:hover {
			color: #24b597;
			/*#208799;*/
		}

		h1,
		h2,
		h3,
		h4,
		h5,
		h6 {
			font-family: 'Caudex-Bold', serif;
		}

		h1 {
			font-size: 24px;
			margin-top: 32px;
			margin-bottom: 8px;
		}

		h2 {
			font-size: 18px;
			margin-top: 12px;
			margin-bottom: 6px;
		}

		table.header {
			font-weight: 300;
			font-size: 17px;
			flex-grow: 1;
			width: 70%;
			max-width: calc(100% - 290px);
			/* Adjust according to the width of .paper-code-tab */
		}

		table td,
		table td * {
			vertical-align: middle;
			position: relative;
		}

		table.paper-code-tab {
			flex-shrink: 0;
			margin-left: 8px;
			margin-top: 8px;
			padding: 0px 0px 0px 8px;
			width: 290px;
			height: 150px;
		}

		.layered-paper {
			/* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
			box-shadow:
				0px 0px 1px 1px rgba(0, 0, 0, 0.35),
				/* The top layer shadow */
				5px 5px 0 0px #fff,
				/* The second layer */
				5px 5px 1px 1px rgba(0, 0, 0, 0.35),
				/* The second layer shadow */
				10px 10px 0 0px #fff,
				/* The third layer */
				10px 10px 1px 1px rgba(0, 0, 0, 0.35);
			/* The third layer shadow */
			margin-top: 5px;
			margin-left: 10px;
			margin-right: 30px;
			margin-bottom: 5px;
		}

		hr {
			height: 1px;
			/* Sets the height of the line to 1 pixel */
			border: none;
			/* Removes the default border */
			background-color: #DDD;
			/* Sets the line color to black */
		}

		div.hypothesis {
			width: 80%;
			background-color: #EEE;
			border: 1px solid black;
			border-radius: 10px;
			-moz-border-radius: 10px;
			-webkit-border-radius: 10px;
			font-family: Courier;
			font-size: 18px;
			text-align: center;
			margin: auto;
			padding: 16px 16px 16px 16px;
		}

		div.citation {
			font-size: 0.8em;
			background-color: #fff;
			padding: 10px;
			height: 200px;
		}

		.fade-in-inline {
			position: absolute;
			text-align: center;
			margin: auto;
			-webkit-mask-image: linear-gradient(to right,
					transparent 0%,
					transparent 40%,
					black 50%,
					black 90%,
					transparent 100%);
			mask-image: linear-gradient(to right,
					transparent 0%,
					transparent 40%,
					black 50%,
					black 90%,
					transparent 100%);
			-webkit-mask-size: 8000% 100%;
			mask-size: 8000% 100%;
			animation-name: sweepMask;
			animation-duration: 4s;
			animation-iteration-count: infinite;
			animation-timing-function: linear;
			animation-delay: -1s;
		}

		.fade-in2-inline {
			animation-delay: 1s;
		}

		.inline-div {
			position: relative;
			display: inline-block;
			/* Makes both the div and paragraph inline-block elements */
			vertical-align: top;
			/* Aligns them at the top, you can adjust this to middle, bottom, etc., based on your needs */
			width: 50px;
			/* Optional: Adds space between the div and the paragraph */
		}

		.image-caption {
			text-align: center;
			font-size: 0.9em;
			margin-top: 5px;
			color: #666;
		}

		.image-caption-small {
			text-align: center;
			font-size: 0.85em;
			margin-top: 5px;
			color: #666;
		}

		ol.references {
			list-style-type: none;
			padding-left: 0;
			counter-reset: ref-counter;
		}

		ol.references li {
			counter-increment: ref-counter;
			margin-bottom: 10px;
			position: relative;
			padding-left: 30px;
		}

		ol.references li::before {
			content: "[" counter(ref-counter) "] ";
			position: absolute;
			left: 0;
			/* font-weight: bold; */
		}
	</style>

	<title>Gaussian Glasses</title>
	<meta property="og:title" content="Gaussian Glasses" />
	<meta charset="UTF-8" />
</head>

<body>
	<div class="content-margin-container">
		<div class="margin-left-block"></div>
		<div class="main-content-block">
			<table class="header;" align="left">
				<tr>
					<td colspan="6">
						<span style="font-size: 32px; font-family: 'Caudex-Bold', serif">
				  			Gaussian Glasses: Do Diffusion Priors Help Classification
				  			Under Corruption and Camouflage?
						</span>
					</td>
				</tr>
				<tr>
					<td align="left">
						<span style="font-size: 17px"><a href="https://hectorastrom.com">Hector Astrom</a></span>
					</td>
					<td align="left">
						<span style="font-size: 17px"><a href="https://kevinbzhu.com">Kevin Zhu</a>
						</span>
					</td>
					<td align="left">
						<span style="font-size: 17px"><a href="http://github.com/luciengaitskell">Luc Gaitskell</a></span>
					</td>
				</tr>

				<tr>
					<td colspan="4" align="left">
						<span style="font-size: 18px">Final project for 6.7960, MIT</span>
					</td>
				</tr>
			</table>
		</div>
		<div class="margin-right-block"></div>
	</div>

	<div class="content-margin-container" id="project_image">
		<div class="margin-left-block">
			<div id="outline-nav">
				<b style="font-size: 16px">Outline</b><br /><br />
				<a href="#intro" style="font-size: 16px">Introduction</a><br /><br />
				<a href="#preliminaries" style="font-size: 16px">Preliminaries</a><br />
				<a href="#preliminaries_diffusion" style="font-size: 14px; margin-left: 15px;">Diffusion</a><br />
				<a href="#preliminaries_pg" style="font-size: 14px; margin-left:
				15px;">RL and Policy Gradient Methods</a><br />
				<a href="#preliminaries_ddpo" style="font-size: 14px; margin-left: 15px;">DDPOTrainer</a><br /><br />
				<a href="#task_data" style="font-size: 16px">Task and Data Description</a><br />
				<a href="#task_data_camouflage" style="font-size: 14px; margin-left: 15px;">COD10K</a><br />
				<a href="#task_data_cifar" style="font-size: 14px; margin-left: 15px;">CIFAR-10-C</a><br />
				<a href="#task_data_imagenet" style="font-size: 14px; margin-left: 15px;">ImageNet-C</a><br /><br />
				<a href="#methods" style="font-size: 16px">Methods</a><br />
				<a href="#methods_imageddpo" style="font-size: 14px; margin-left: 15px;">ImageDDPO</a><br />
				<a href="#methods_rl" style="font-size: 14px; margin-left: 15px;">RL Methods</a><br />
				<a href="#methods_design" style="font-size: 14px; margin-left: 15px;">Design Choices</a><br />
				<a href="#methods_sft" style="font-size: 14px; margin-left: 15px;">Supervised Fine-Tuning (Kevin)</a><br />
				<a href="#methods_benchmarking" style="font-size: 14px; margin-left: 15px;">Evaluation</a><br /><br />
				<a href="#results" style="font-size: 16px">Results</a><br />
				<a href="#results_camouflage" style="font-size: 14px; margin-left: 15px;">Camouflage Task</a><br />
				<a href="#results_cifar" style="font-size: 14px; margin-left:
				15px;">Corruption Task</a><br /><br />
				<a href="#discussion" style="font-size: 16px">Discussion</a><br />
				<a href="#discussion_failure_analysis" style="font-size: 14px; margin-left: 15px;">Failure Analysis</a><br />
				<a href="#discussion_extensions" style="font-size: 14px; margin-left: 15px;">Extensions</a><br /><br />
				<a href="#conclusion" style="font-size: 16px">Conclusion</a><br /><br />
			</div>
		</div>
		<div class="main-content-block">
			<img src="./images/overfit_batfish.png" style="width: 80%; height: auto;" />
			<p class="heavy-caption">Example of overfitting in RL-trained model</p>
		</div>
		<div class="margin-right-block"><strong id="figure_1">Figure 1</strong> <br><br>With
		only a single image, Stable Diffusion trained with ImageDDPO can quickly
		overfit to generate its best idea of the prototypical 'batfish'.</div>
	</div>

	<div class="content-margin-container" id="intro">
		<div class="margin-left-block"></div>
		<div class="main-content-block">
			<h1>Introduction (Hector)</h1>
			Since the emergence of Denoising Diffusion Probabilistic Models
			(DDPMs), generative vision has evolved from simple synthesis to
			precise, instructional editing <a href="#ref_ho" class="cite"></a> <a href="#ref_ho_cfg" class="cite"></a>. Models like
			InstructPix2Pix <a href="#ref_brooks" class="cite"></a> and
			ControlNet <a href="#ref_zhang" class="cite"></a> have demonstrated that diffusion
			priors can manipulate
			image semantics with remarkable fidelity, effectively
			"understanding" the contents of a scene to modify it.
			<br><br>
			
			This progress motivates a central question in robust computer
			vision: <b>can a diffusion prior be used as a label-preserving
			semantic denoiser for recognition?</b> Concretely, we ask whether
			diffusion-based reconstruction can restore discriminative structure
			in ambiguous or corrupted inputs, improving downstream
			classification by transforming the <i>data</i> rather than modifying
			the classifier.
			<br><br>
			
			In this work, we investigate whether Stable Diffusion v1.5 <a href="#ref_rombach" class="cite"></a>,
			optimized through both Supervised Fine-Tuning (SFT) and
			Reinforcement Learning (DDPO <a href="#ref_black" class="cite"></a>), can improve the zero-shot
			classification performance of CLIP <a href="#ref_radford" class="cite"></a> on two distinct domains of visual
			degradation:
			<ul>
				<li>
					<strong>Natural Ambiguity:</strong> The COD10K (Camouflaged Object
					Detection) dataset <a href="#ref_fan" class="cite"></a>,
					where the signal (animal) is hidden or camouflaged.
				</li>
				<li>
					<strong>Algorithmic Corruption:</strong> The CIFAR-10-C and ImageNet-C datasets <a href="#ref_hendrycks" class="cite"></a>, representing
					algorithmic noise and blur degradations to CIFAR and ImageNet images, which we only explore in preliminary experiments
					that did not reach stable performance within our compute budget.
				</li>
			</ul>
			
			We report a negative result on COD10K: our SFT- and RL-based attempts do not yield a statistically significant
			improvement over the base CLIP accuracy. For CIFAR-10-C, our experiments remained preliminary — the model did not
			produce stable or interpretable generations within the project timeline — so we do not draw quantitative conclusions
			for that setting.
			
			To read more about our hypotheses for the failure modes, refer to
			the <a href="#discussion_failure_analysis">discussion.</a>

			
			<br><br>

			Given the substantial computational complexity and latency
			introduced by the diffusion loop, we conclude that, for our
			completed COD10K experiments — and for preliminary CIFAR-10-C runs
			that failed to yield stable improvements — generative semantic
			completion is an ineffective approach for improving robustness to
			visually degraded inputs compared to standard classifier baselines.

			<br><br>
			All code for this experiement is available <a
			href="https://github.com/hectorastrom/diffusion-lens">here.</a>
			For readers to build on this work, we also provide a standalone
			package implementing ImageDDPO available with <code>pip
			install imageddpo</code>.
		</div>
		<div class="margin-right-block">
		</div>
	</div>

	<div class="content-margin-container" id="preliminaries">
		<div class="margin-left-block"></div>
		<div class="main-content-block">
			<h1>Preliminaries (Kevin + Hector)</h1>
			<!-- NOTE: Merging related work into this section; just cite a lot -->
			<h2 id="preliminaries_diffusion">Diffusion (Kevin)</h2>
			<p>
			In this work, we adopt <strong>conditional diffusion probabilistic models</strong>, which define a distribution
			<a href="#ref_sohldickstein" class="cite"></a> <a href="#ref_ho" class="cite"></a>, which define a distribution
				\( p(x_0 | c) \) over data samples \( x_0 \) given an associated context \( c \).
				These models operate by introducing a forward noising process
				\( q(x_t | x_{t-1}) \), a Markov chain that incrementally corrupts the data with noise across many
				timesteps. The learning task is to approximate the reverse of this corruption process.
			</p>

			<p>
				To achieve this, a neural network \( \mu_\theta(x_t, c, t) \) is trained to predict the posterior mean
				of the forward process at each timestep. Training proceeds by sampling a data pair
				\( (x_0, c) \), selecting a timestep \( t \) uniformly, and generating a noisy latent
				\( x_t \) using the forward diffusion kernel. The model minimizes the difference between
				its prediction and the true posterior mean \( \tilde{\mu}(x_0, t) \):
			</p>

			$$ L_{\text{DDPM}}(\theta) = \mathbb{E} \left[ \| \tilde{\mu}(x_0, t) - \mu_\theta(x_t, c, t) \|^2 \right].
			$$

			<p>
				This objective essentially maximizes a variational lower bound on the data
				log-likelihood, which grounds the procedure in a principled generative modeling framework.
				Because the forward process is fixed and analytically tractable, the model’s task is to
				estimate the reverse denoising dynamics.
			</p>

			<p>
				At sampling time, generation begins from Gaussian noise \( x_T \sim \mathcal{N}(0, I) \). The model then
				iteratively applies the learned reverse transitions
				\( p_\theta(x_{t-1} | x_t, c) \), gradually removing noise and steering the sample
				toward the data distribution. Most widely used samplers <a href="#ref_ho" class="cite"></a> <a href="#ref_song" class="cite"></a> implement
				this reverse update as an isotropic Gaussian with a timestep-dependent variance:
			</p>

			$$ p_\theta(x_{t-1} | x_t, c) = \mathcal{N}(x_{t-1} | \mu_\theta(x_t, c, t), \sigma_t^2 I). $$

			<p>
				By chaining these denoising steps together, diffusion models have shown the ability to generate
				high-quality samples using an
				iterative, noise-to-data trajectory that is fully guided by the learned model.
			</p>

			<h2 id="preliminaries_pg">Reinforcement Learning and Policy Gradient
			Methods (Hector)</h2>
			This work also explores the use of reinforcement learning to
			optimize the diffusion model via the DDPO algorithm, enabling
			optimization around a hypothetical black box classifier. 
			<br><br>
			<strong>Reinforcement Learning</strong> is a type of optimization
			framework whereby an agent learns to make decisions (via a policy)
			in an environment to maximize cumulative reward. As framed, the
			agent does not need to understand everything about its environment —
			nor any details about the reward function’s implementation — in
			order to maximize it. This enables a) black box optimization and b)
			optimization of arbitrary, non-differentiable objectives.
			<br><br>
			
			<strong>Policy Gradient Methods</strong> <a href="#ref_pg_intro"
			class="cite"></a> are a class of reinforcement learning algorithms
			that maximize expected reward by directly optimizing policy
			parameters. Policy gradient methods require two core components:
			
			<ol>
				<li>A stochastic policy \( \pi_\theta \) (e.g. a neural net with
				probabilistic outputs)</li>
				<li>An advantage estimator \( \hat{A}_t \) (measuring how good
				an action was compared to average)
			</ol>
			
			The standard objective policy gradient methods seek to maximize is:
			
			$$ \mathcal{J}^{\text{PG}}(\theta) = \hat{\mathbb{E}}_t[\log
			\pi_\theta (a_t|s_t) \hat{A}_t] $$
			
			\( \pi_\theta(a_t|s_t) \) is the current policy, trained to predict
			action \( a_t \) from state \( s_t \). The Advantage function \(
			\hat{A}_t \) <a href="#ref_schulman_gae" class="cite"></a>
			quantifies how much better a specific action was compared to the
			"average" action in that state and acts as a stable training signal
			(compared to reward).
			<br><br>

			In code (e.g. PyTorch) we implement this using autograd by
			minimizing the negative objective: \(
			\mathcal{L}^{\text{PG}}=-\mathcal{J}^{\text{PG}} \).
			<br><br>

			<strong>PPO (Proximal Policy Optimization)</strong> extends policy
			gradient methods by clipping the probability ratio between the new
			and old policy, effectively building a trust region for the range of
			allowed policy updates on each optimization step <a
			href="#ref_schulman_ppo" class="cite"></a>.

			<h2 id="preliminaries_ddpo">DDPOTrainer (Hector)</h2>
			<strong>DDPO</strong> <a href="#ref_black" class="cite"></a> frames
			the diffusion model as an agent in an environment, where the action
			is the diffusion step, the state is the partially denoised image +
			text promptconditioning, and the reward can be any function (e.g.
			aesthetic quality, compressibility, classification accuracy, etc.). 

			<br><br>

			The HuggingFace TRL library <a href="#ref_trl_ddpo" class="cite"></a>
			provides a (now
			deprecated) DDPOTrainer
			class to implement this learning algorithm. At a high level it works
			as follows:
			<ol>
				<li>Initialize trainer with pipeline,
				scheduler, reward, config.</li>
				<li>Sample trajectories by running the
				diffusion process and recording actions + log-probs.</li>
				<li>Decode final latents to images.</li>
				<li>Compute rewards for each sample.</li>
				<li>Normalize advantages globally or
				per-prompt.</li>
				<li>Compute PPO loss using replayed log-probs +
				current policy.</li>
				<li>Update UNet via LoRA, keeping scheduler +
				VAE fixed.</li>
				<li>Repeat for many epochs (sample → PPO →
				update).</li>
			</ol>

			In this work, we adapt the DDPOTrainer class to support image
			inputs, so that the Stable Diffusion can be used as an image editor,
			rather than just image generators. In the <a href="#methods_imageddpo">methods</a> section, we'll
			discuss the modifications we made to the DDPOTrainer class. 
		</div>
		<div class="margin-right-block">
		</div>
	</div>

	<div class="content-margin-container" id="task_data">
		<div class="margin-left-block"></div>
		<div class="main-content-block">
			<h1>Task and Dataset Details (Luc)</h1>
			We looked for tasks where diffusion could 'assist' the classifier,
			by clearing out noise or enhancing the signal.
			Therefore, we looked for tasks with visually occluded images.
			
			<h2 id="task_data_camouflage">Camouflage Removal on COD10K</h2>
			One primary example of this is the COD10K dataset <a href="#ref_fan" class="cite"></a>,
			which contains images of camouflaged animals in natural settings.
			These images can confuse the classifier, as the animals are
			often blended into the background, such as the deer in the
			brush as can be seen below.
		</div>
		<div class="margin-right-block"></div>
	</div>

	<div class="content-margin-container">
		<div class="margin-left-block"></div>
		<div class="main-content-block">
			<img src="./images/cod10k-example-deer.jpg" style="width: 80%; height: auto;" />
			<p class="heavy-caption">COD10K Sample Image (deer)</p>
		</div>
		<div class="margin-right-block"><strong>Figure 2</strong></div>
	</div>

	<div class="content-margin-container">
		<div class="margin-left-block"></div>
		<div class="main-content-block">
			The goal is to use diffusion to enhance the visibility and
			detectability of these camouflaged animal,
			thereby increasing the accuracy of a frozen classifier.
			Additionally, the result of the diffusion would likely
			be very interpretable, highlighting the areas of the image
			that correspond to the animal.
			
			<h2 id="task_data_cifar">Corruption Removal on CIFAR-10-C</h2>
			Another task we explored is corruption removal on the CIFAR-10-C
			dataset <a href="#ref_hendrycks" class="cite"></a>, which contains
			images from CIFAR-10 that have been corrupted with various
			types of noise and blur, including Gaussian noise, motion blur.
			An example image for the truck label, corrupted with gaussian
			noise is shown below.
		</div>
		<div class="margin-right-block"></div>
	</div>

	<div class="content-margin-container">
		<div class="margin-left-block"></div>
		<div class="main-content-block">
			<img src="./images/cifar10c-example-truck-gaussian-noise.png" style="width: 50%; height: auto;" />
			<p class="heavy-caption">CIFAR-10-C Sample Image (Truck corrupted with Gaussian noise)</p>
		</div>
		<div class="margin-right-block"><strong>Figure 3</strong></div>
	</div>

	<div class="content-margin-container">
		<div class="margin-left-block"></div>
		<div class="main-content-block">
			The dataset was accessed from the <a href="https://github.com/hendrycks/robustness">paper's GitHub repository</a>.
			
			To assess the baseline performance of CLIP on the different
			noise types, we evaluated its zero-shot classification accuracy
			on each corruption category. The results for the CIFAR-10-C
			corruption types we considered, with the highest severity (5),
			are summarized in the following table:
			
			<table style="border-collapse: collapse; width: 100%; margin: 20px 0;">
				<thead>
					<tr style="border-bottom: 2px solid #333;">
						<th style="padding: 10px; text-align: left;">CIFAR-10C
						Corruption Type (Severity 5)</th>
						<th style="padding: 10px; text-align: center;">CLIP Accuracy (%)</th>
					</tr>
				</thead>
				<tbody>
					<tr style="border-bottom: 1px solid #ddd;">
						<td style="padding: 10px;">Glass Blur</td>
						<td style="padding: 10px; text-align: center;">43.0</td>
					</tr>
					<tr style="border-bottom: 1px solid #ddd;">
						<td style="padding: 10px;">Gaussian Noise</td>
						<td style="padding: 10px; text-align: center;">43.0</td>
					</tr>
					<tr style="border-bottom: 1px solid #ddd;">
						<td style="padding: 10px;">Speckle Noise</td>
						<td style="padding: 10px; text-align: center;">50.0</td>
					</tr>
					<tr style="border-bottom: 1px solid #ddd;">
						<td style="padding: 10px;">Shot Noise</td>
						<td style="padding: 10px; text-align: center;">48.0</td>
					</tr>
					<tr style="border-bottom: 1px solid #ddd;">
						<td style="padding: 10px;">Elastic Transform</td>
						<td style="padding: 10px; text-align: center;">56.0</td>
					</tr>
					<tr style="border-bottom: 1px solid #ddd;">
						<td style="padding: 10px;">Pixelate</td>
						<td style="padding: 10px; text-align: center;">58.0</td>
					</tr>
					<tr style="border-bottom: 1px solid #ddd;">
						<td style="padding: 10px;">Contrast</td>
						<td style="padding: 10px; text-align: center;">62.0</td>
					</tr>
					<tr style="border-bottom: 1px solid #ddd;">
						<td style="padding: 10px;">Impulse Noise</td>
						<td style="padding: 10px; text-align: center;">61.0</td>
					</tr>
					<tr style="border-bottom: 1px solid #ddd;">
						<td style="padding: 10px;">JPEG Compression</td>
						<td style="padding: 10px; text-align: center;">60.0</td>
					</tr>
					<tr style="border-bottom: 1px solid #ddd;">
						<td style="padding: 10px;">Gaussian Blur</td>
						<td style="padding: 10px; text-align: center;">69.0</td>
					</tr>
					<tr>
						<td style="padding: 10px;">Motion Blur</td>
						<td style="padding: 10px; text-align: center;">70.0</td>
					</tr>
				</tbody>
			</table>
			
			In general, the small size of the image and the resulting severity
			of the corruptions makes the images difficult to interpret.
			However, the nature of diffusion models as denoisers suggests that
			the diffusion prior could help recover some of the lost structure,
			improving classification accuracy.
			
			One particular concern with the CIFAR-10 dataset,
			is the significantly smaller image size compared to the usual
			input dimensions to stable diffusion (32x32 vs 224x224).
			
			
			<h2 id="task_data_imagenet">Corruption Removal on ImageNet-C</h2>
			The address this concern, we also explored corruption removal
			on the ImageNet-C dataset <a href="#ref_hendrycks" class="cite"></a>,
			which contains images from ImageNet that have been corrupted.
			Similar to CIFAR-10-C, the corruptions include Gaussian noise,
			motion blur, and glass blur. An example of an image labeled as
			"ostrich, Struthio camelus", corrupted with max severity (5) fog
			is shown below.
		</div>
		<div class="margin-right-block"></div>
	</div>

	<div class="content-margin-container">
		<div class="margin-left-block"></div>
		<div class="main-content-block">
			<img src="./images/imagenetc-example-ostrich-fog-severity5.jpeg" style="width: 50%; height: auto;" />
			<p class="heavy-caption">ImageNet-C Sample Image (Ostrich with severity 5 fog)</p>
		</div>
		<div class="margin-right-block" id="figure_4"><strong>Figure 4</strong></div>
	</div>

	<div class="content-margin-container">
		<div class="margin-left-block"></div>
		<div class="main-content-block">
			The dataset was accessed from the <a href="https://github.com/hendrycks/robustness">paper's GitHub repository</a>.
			The labels were not provided in the ImageNet-C dataset, but are the standard
			ImageNet-1000 labels and were downloaded separately from <a href="https://www.image-net.org/download.php">ImageNet</a>.
			
			Again, like with CIFAR-10-C, to assess the baseline performance
			of CLIP on each candidate noise type, we evaluated its zero-shot.
			The results for the ImageNet-C corruption types we considered,
			with the highest severity (5), are summarized in the following table:
			
			<table style="border-collapse: collapse; width: 100%; margin: 20px 0;">
				<thead>
					<tr style="border-bottom: 2px solid #333;">
						<th style="padding: 10px; text-align: left;">ImageNet-C Corruption Types (Severity 5)</th>
						<th style="padding: 10px; text-align: center;">CLIP Accuracy (%)</th>
					</tr>
				</thead>
				<tbody>
					<tr style="border-bottom: 1px solid #ddd;">
						<td style="padding: 10px;">Impulse Noise</td>
						<td style="padding: 10px; text-align: center;">8.9%</td>
					</tr>
					<tr style="border-bottom: 1px solid #ddd;">
						<td style="padding: 10px;">Glass Blur</td>
						<td style="padding: 10px; text-align: center;">10.1%</td>
					</tr>
					<tr style="border-bottom: 1px solid #ddd;">
						<td style="padding: 10px;">Gaussian Noise</td>
						<td style="padding: 10px; text-align: center;">14.1%</td>
					</tr>
					<tr style="border-bottom: 1px solid #ddd;">
						<td style="padding: 10px;">Shot Noise</td>
						<td style="padding: 10px; text-align: center;">16.6%</td>
					</tr>
					<tr style="border-bottom: 1px solid #ddd;">
						<td style="padding: 10px;">Fog</td>
						<td style="padding: 10px; text-align: center;">25.1%</td>
					</tr>
					<tr style="border-bottom: 1px solid #ddd;">
						<td style="padding: 10px;">Pixelate</td>
						<td style="padding: 10px; text-align: center;">28.4%</td>
					</tr>
					<tr style="border-bottom: 1px solid #ddd;">
						<td style="padding: 10px;">Elastic Transform</td>
						<td style="padding: 10px; text-align: center;">29.7%</td>
					</tr>
					<tr style="border-bottom: 1px solid #ddd;">
						<td style="padding: 10px;">Zoom Blur</td>
						<td style="padding: 10px; text-align: center;">31.8%</td>
					</tr>
					<tr style="border-bottom: 1px solid #ddd;">
						<td style="padding: 10px;">Snow</td>
						<td style="padding: 10px; text-align: center;">35.9%</td>
					</tr>
					<tr style="border-bottom: 1px solid #ddd;">
						<td style="padding: 10px;">Motion Blur</td>
						<td style="padding: 10px; text-align: center;">36.5%</td>
					</tr>
					<tr style="border-bottom: 1px solid #ddd;">
						<td style="padding: 10px;">JPEG Compression</td>
						<td style="padding: 10px; text-align: center;">38.2%</td>
					</tr>
					<tr>
						<td style="padding: 10px;">Frost</td>
						<td style="padding: 10px; text-align: center;">39.3%</td>
					</tr>
				</tbody>
			</table>
			
			Having a larger image size, while still also having
			a challenging but more noise-like corruption profile,
			makes this dataset a good candidate for diffusion-based
			denoising to help classification.
		</div>
		<div class="margin-right-block"></div>
	</div>

	<div class="content-margin-container" id="methods">
		<div class="margin-left-block"></div>
		<div class="main-content-block">
			<h1>Methods</h1>
		</div>
		<div class="margin-right-block"></div>
	</div>

	<div class="content-margin-container">
		<div class="margin-left-block"></div>
		<div class="main-content-block">
			<img src="./images/train_pipelines.png" style="width: 80%; height: auto;" />
			<p class="heavy-caption">SFT and RL training pipelines</p>
		</div>
		<div class="margin-right-block"><strong>Figure 5</strong><br /><br />In the SFT pipeline, gradient updates are free to backpropagate through a
		frozen classifier. Conversely, in RL parameter updates are faciliated
		through DDPO, where the reward is a weighted combination of classifier
		confidence and perceptual drift. </div>
	</div>

	<div class="content-margin-container">
		<div class="margin-left-block"></div>
		<div class="main-content-block">
			<h2 id="methods_imageddpo">ImageDDPO</h2>
			
			<p>
				ImageDDPO extends DDPO to the img2img setting by optimizing a diffusion policy that edits an input image instead of
				generating purely from noise.
			</p>
			
			<p><strong>MDP and Objective Changes</strong></p>
			<ul>
				<li>
					<strong>Multimodal context.</strong>
					The conditioning becomes
					\( c = (\text{text_prompt},\, \text{input_image}) \),
					so the policy learns edits relative to a source image.
				</li>
				<li>
					<strong>Non-noise initial state.</strong>
					Instead of \( x_T \sim \mathcal{N}(0, I) \),
					we encode the input image, then apply forward diffusion to reach
					\( x_{sT} \) with noise strength \( s \in [0,1] \).
				</li>
				<li>
					<strong>Shortened horizon.</strong>
					The denoising trajectory runs from \( t = sT \) down to \( t = 0 \)
					(e.g. \( s = 0.4 \Rightarrow t = 400 \to 0 \)),
					but each transition is still a Gaussian
					\( p_\theta(x_{t-1} \mid x_t, c) \),
					so DDPO’s log-prob and PPO machinery apply unchanged over this truncated chain.
				</li>
			</ul>
			
			<p><strong>Implementation Changes from DDPO</strong></p>
			<ul>
				<li>
					<strong>Img2img DDPO pipeline.</strong>
					<code>Img2ImgDDPOStableDiffusionPipeline</code> wraps the base DDPO pipeline to:
					(i) VAE-encode the input image, (ii) forward diffuse to \( x_{sT} \),
					and (iii) denoise from \( t = sT \) to \( 0 \) while recording latents and log-probs.
				</li>
				<li>
					<strong>Noise strength hyperparameter.</strong>
					A scalar \( s \in [0,1] \) controls how far back we start in the diffusion chain,
					defining the effective MDP horizon and the set of actions included in the PPO loss.
				</li>
				<li>
					<strong>Prompt and reward interfaces.</strong>
					The prompt function yields tuples
					\( (\text{init_images},\, \text{text_prompts},\, \text{metadata}) \),
					and reward functions are upgraded to
					\( r(x_0, c) = r(x_0, \text{init_image}, \text{text_prompt}, \text{metadata}) \),
					enabling image-conditioned objectives (e.g. "make the animal more detectable").
				</li>
			</ul>
			
			<h2 id="methods_rl">RL Methods</h2>
			<p>
				Our reinforcement learning setup extends the <code>ImageDDPOTrainer</code> with three
				task-specific modifications. First, we define a reward function equal to the logit
				difference between the optimized and original images, which yielded the most stable
				gradient signal compared to margin-based or probability-based alternatives. Second,
				we integrate distributed training via <code>accelerate</code>, allowing rollouts, reward
				evaluation, and PPO updates to run efficiently across devices. Third, we add a
				validation hook that periodically reconstructs diffusion intermediates to monitor
				whether training meaningfully affects the denoising trajectory;
				<a href="#figure_1">Figure 1</a> shows an
				example intermediate.
			</p>
			
			<p>
				Beyond these changes, the training loop remains identical: once images were
				preprocessed into the COD10K format, the same <code>ImageDDPOTrainer</code> was used to
				fine-tune Stable Diffusion on CIFAR-10-C and ImageNet-C. Because both datasets
				provide plain class labels, the reward definition, sampling procedure, and PPO
				update remain unchanged. CIFAR-10-C’s smaller spatial resolution and 10-class
				structure make optimization comparatively easy, while ImageNet-C’s 1000-class
				label space introduces a noisier reward landscape that demands more rollouts to
				achieve consistent improvement.
			</p>

			<h2 id="methods_design">Design Choices</h2>

			<p>
				The core design choices we confronted early in the development of
				this work were:
			</p>
			<ol>
				<li>
					<strong>Vision Backbone (CLIP).</strong>
					We selected CLIP as the downstream classifier because its
					open-vocabulary interface allows a single model to evaluate
					any dataset without retraining. This enabled rapid
					experimentation with prompt engineering for both evaluation
					and generation, and aligned naturally with Stable
					Diffusion’s text-conditioning pipeline. Specifically, we used
					<code>openai/clip-vit-base-patch16</code> because its smaller
					patch size measurably improved accuracy and reward sensitivity
					on camouflaged scenes. 
					
					<br><br>

					For CIFAR-10-C and ImageNet-C, a
					lightweight supervised classifier (e.g., ResNet-18/50) would
					have been better matched to the label distributions.
					However, these vision models require training a final classifier
					layer for each dataset.
					To match our demonstration with CLIP, we would train the
					classifier from the original non-corrupted training set
					(CIFAR-10 or ImageNet).
					This would then allow us to evaluate the diffusion model's
					ability to improve classification accuracy on corrupted inputs.
					However, time and resource constraints prevented a full comparison.
				</li>
			
				<li>
					<strong>Generative Backbone (Stable Diffusion 1.5).</strong>
					We adopted SD 1.5 for its balance of generation quality and
					inference speed — both essential in an RL loop that
					repeatedly samples full diffusion trajectories. SD 1.5 also
					integrates cleanly into the existing
					<code>DDPOTrainer</code> codepath, minimizing engineering
					overhead.
				</li>
			
				<li>
					<strong>Reward Function.</strong>
					We evaluated several formulations and found that only
					logit-difference rewards produced stable, monotonic
					learning. Probability-based rewards were too flat to guide
					improvement, and margin-style rewards exhibited volatile
					scale. Logit change provided a self-referential signal with
					consistent magnitude across samples.
				</li>
			
				<li>
					<strong>Reconstruction Metric.</strong>
					We used LPIPS to quantify perceptual drift between the input
					and reconstructed output. Compared to pixelwise MSE or
					KL-based metrics, LPIPS more reliably tracks semantic
					preservation — the quantity we most wanted to penalize when
					encouraging classification-helpful edits. 
					
					<br><br>
					Empirically, however, this penalty was too aggressive:
					because we never actually reached a reward-hacking regime,
					the LPIPS term dominated early updates and drove rewards
					sharply negative before useful behavior could emerge.
					<br><br>
				</li>
			</ol>




			<h2 id="methods_sft">Supervised Fine-Tuning (Kevin)</h2>
			<p>
				Our initial approach used RL via DDPO, which provides a general framework for optimizing 
				diffusion models with respect to arbitrary reward functions, including non-differentiable 
				objectives and black-box classifiers. However, our RL results did not show significant 
				improvement over baselines, motivating us to explore a supervised fine-tuning (SFT) method 
				that directly backpropagates gradients through the entire diffusion pipeline. This approach 
				eliminates the variance inherent in policy gradient methods by computing exact gradients 
				through the differentiable components of the system. We pursued SFT because our specific 
				setup, where CLIP classification is fully differentiable, allows us to compute exact 
				gradients, potentially leading to more stable training and faster convergence than policy 
				gradient methods. While RL remains the more general solution that can handle 
				non-differentiable objectives, SFT offers a lower-variance alternative when the reward 
				function is differentiable.
			</p>

			<p>
			Our SFT method trains a LoRA adapter on the UNet component of Stable Diffusion
			<a href="#ref_rombach" class="cite"></a> to optimize CLIP <a href="#ref_radford" class="cite"></a> classification accuracy. The key difference from RL is that 
			SFT requires maintaining the full computational graph through the entire pipeline: we 
			backpropagate gradients not only through the UNet (as RL does) but also through the VAE 
			decoder and frozen CLIP classifier to compute exact gradients with respect to the 
			classification loss. This requires using deterministic DDIM sampling (η = 0) to maintain 
			gradient flow, whereas RL can use stochastic sampling and only needs gradients through 
			the UNet to compute policy gradients. Additionally, SFT requires a differentiable objective 
			function (CLIP's classification loss), while RL can optimize any reward signal, including 
			non-differentiable ones.
			</p>

			<h3>Architecture and Pipeline</h3>
			<p>
				The training pipeline follows this forward pass:
			</p>

			$$
			\begin{gathered}
			\text{Image} \to \text{VAE Encode} \to \text{Add Noise}
			\to \text{Denoise w/ UNet (LoRA)} \to \text{VAE Decode} \to \text{CLIP} \to \text{Classification Loss}
			\end{gathered}
			$$

			<p>
				We freeze all components except the LoRA parameters applied to the UNet's
				attention layers. Specifically, LoRA adapters are applied to the query, key,
				value, and output projection matrices (<code>to_q</code>, <code>to_k</code>,
				<code>to_v</code>, <code>to_out.0</code>) in all cross-attention and
				self-attention blocks. We use a LoRA rank of 4 and alpha of 4, resulting in
				only a small fraction of the UNet's parameters being trainable (less than 1% of the full model). The VAE
				encoder and decoder, text encoder,
				and CLIP classifier are all frozen, ensuring that gradients flow only through
				the LoRA-adapted UNet.
			</p>

			<h3>Differentiable Denoising Loop</h3>
			<p>
				The core of our approach is a fully differentiable denoising loop implemented
				using DDIM sampling with \( \eta = 0 \) (deterministic mode). This ensures
				that the reverse diffusion process is deterministic and maintains gradient
				flow throughout all denoising steps. For each training sample:
			</p>

			<ol>
				<li>
					<strong>VAE Encoding:</strong> The input image is encoded into the latent
					space using the frozen VAE encoder, producing latents \( z_0 \).
				</li>
				<li>
					<strong>Noise Injection:</strong> We apply image-to-image (I2I) conditioning
					by adding noise to the latents. Given a noise strength parameter
					\( \alpha \) (typically 0.4), we select a timestep \( t_{\text{start}} \)
					corresponding to the \( (1 - \alpha) \) quantile of the diffusion schedule,
					then add noise: \( z_{t_{\text{start}}} = \sqrt{1-\beta_t} z_0 + \sqrt{\beta_t} \epsilon \),
					where \( \epsilon \sim \mathcal{N}(0, I) \).
				</li>
				<li>
					<strong>Differentiable Denoising:</strong> Starting from \( z_{t_{\text{start}}} \),
					we run a deterministic DDIM denoising loop for the remaining
					\( \alpha \times \text{num_steps} \) steps. At each timestep \( t \), the UNet
					predicts noise \( \epsilon_\theta(z_t, c, t) \), where \( c \)
					is the text conditioning. We optionally apply classifier-free guidance (CFG)
					by computing both unconditional and conditional predictions and combining
					them: \( \epsilon_{\text{pred}} = \epsilon_{\text{uncond}} + \gamma(\epsilon_{\text{cond}} -
					\epsilon_{\text{uncond}}) \),
					where \( \gamma \) is the guidance scale. The DDIM step update is:
				</li>
			</ol>

			$$ z_{t-1} = \sqrt{\frac{\alpha_{t-1}}{\alpha_t}} z_t + \left( \sqrt{1-\alpha_{t-1}} -
			\sqrt{\frac{\alpha_{t-1}}{\alpha_t}} \sqrt{1-\alpha_t} \right) \epsilon_{\text{pred}} $$

			<p>
				All operations in this loop maintain gradient flow, allowing
				backpropagation through the entire sequence of denoising steps.
			</p>

			<h3>Classification Loss and Training</h3>
			<p>
				After denoising, the latents are decoded back to pixel space using the frozen
				VAE decoder, producing generated images \( \hat{x} \). These images are then
				preprocessed for CLIP (resized to 224×224 and normalized using CLIP's
				statistics) and passed through the frozen CLIP vision encoder to obtain
				normalized image features \( f_{\text{img}} \).
			</p>

			<p>
				We pre-compute normalized text embeddings \( f_{\text{text}}^{(i)} \)
				for all classes in the dataset using prompts "An image of {class_name}" for each class.
				Classification logits are computed as:
			</p>

			$$ \text{logits}_i = \exp(\tau) \times (f_{\text{img}} \cdot f_{\text{text}}^{(i)}) $$

			<p>
				where \( \tau \) is CLIP's learned temperature parameter. We minimize the
				cross-entropy loss between these logits and the ground-truth class labels
				using standard backpropagation.
			</p>

			<h3>Experiments</h3>
			<p>
				Training uses mixed precision (FP16 for diffusion components, FP32 for CLIP)
				and gradient accumulation to achieve an effective batch size of 16 (batch
				size 2 × 8 accumulation steps). We use the AdamW optimizer with a learning
				rate of 1e-4. The diffusion process uses 20 total steps with a noise strength
				of 0.4, meaning we denoise only the final 40% of the diffusion schedule.
				We use a guidance scale of 1.0 for all the SFT experiments.
				This I2I approach allows the model to make targeted modifications to the
				input image while preserving its overall structure.
			</p>

			<p>
				We experiment with different prompt strategies: (1) ORACLE mode,
				where prompts use the ground-truth class label (e.g., "A clear photo of {class}"),
				(2) custom fixed prompts, and (3) empty prompts. The ORACLE setting provides
				an upper bound on performance by giving the model perfect semantic information,
				while empty prompts test whether the model can learn to improve classification
				without explicit text guidance.
			</p>

			<p>
				For the custom fixed prompts, we use the following prompts:
			<ul>
				<li>"De-camouflage the animal" — a direct semantic instruction to remove camouflage effects, testing
					whether explicit task guidance helps preserve the correct object.</li>
				<li>"Increase edge sharpness and contrast of the camouflaged animal" — focuses on low-level visual
					features rather than high-level semantics, testing whether edge enhancement alone improves
					classification.</li>
				<li>"Enhance the visibility of the camouflaged animal" — a general instruction that leaves the method of
					enhancement unspecified, testing whether vague guidance is sufficient.</li>
			</ul>
			</p>

			<h2 id="methods_benchmarking">Evaluation</h2>
			<p>
				To evaluate the effectiveness of our diffusion-enhanced classification approach, we compare four
				conditions on the COD10K test set.
				We establish two baselines: direct zero-shot classification of original images using CLIP without
				diffusion processing, and classification
				of images processed through the base (untrained) Stable Diffusion model. We then evaluate our trained
				models: RL-trained SD and SFT-trained SD,
				both fine-tuned for the camouflage classification task.
			</p>

			<p>
				All evaluations use the same CLIP model (<code>openai/clip-vit-base-patch16</code>)
				and preprocessing pipeline to ensure fair comparison. Images are resized to
				224×224 using bicubic interpolation and normalized using CLIP's standard
				statistics (mean: [0.4815, 0.4578, 0.4082], std: [0.2686, 0.2613, 0.2758]).
				We pre-compute normalized text embeddings for all classes using prompts of the
				form "An image of {class_name}" and compute classification logits using CLIP's
				learned temperature-scaled cosine similarity.
			</p>

			<p>
				For each condition, we report top-1 through top-5 classification accuracy,
				mean negative log-likelihood (NLL), and mean probability assigned to the
				correct class. These metrics are computed across the entire test set and
				optionally broken down by class for per-class analysis.
			</p>

		</div>
		<div class="margin-right-block"></div>
	</div>

	<div class="content-margin-container" id="results">
		<div class="margin-left-block"></div>
		<div class="main-content-block">
			<h1>Results</h1>

			<h2 id="results_camouflage">Camouflage Task</h2>
			<p>
				<a href="#table_1">Table 1</a> shows classification accuracy results on the COD10K test set across
				four experimental conditions: CLIP baseline, BASE SD + CLIP, RL-trained SD + CLIP,
				and SFT-trained SD + CLIP.
			</p>

			<table id="table_1" style="border-collapse: collapse; width: 100%; margin: 20px 0;">
				<thead>
					<tr style="border-bottom: 2px solid #333;">
						<th style="padding: 10px; text-align: left;">Method</th>
						<th style="padding: 10px; text-align: center;">Top-1 Accuracy</th>
						<th style="padding: 10px; text-align: center;">Top-3 Accuracy</th>
						<th style="padding: 10px; text-align: center;">Top-5 Accuracy</th>
					</tr>
				</thead>
				<tbody>
					<tr style="border-bottom: 1px solid #ddd;">
						<td style="padding: 10px;"><strong class="extrabold">CLIP Baseline</strong></td>
						<td style="padding: 10px; text-align: center;"><strong class="extrabold">44.57%</strong></td>
						<td style="padding: 10px; text-align: center;"><strong class="extrabold">68.51%</strong></td>
						<td style="padding: 10px; text-align: center;"><strong class="extrabold">77.00%</strong></td>
					</tr>
					<tr style="border-bottom: 1px solid #ddd;">
						<td style="padding: 10px;"><strong>BASE SD + CLIP (Null Prompt)</strong></td>
						<td style="padding: 10px; text-align: center;">11.40%</td>
						<td style="padding: 10px; text-align: center;">26.60%</td>
						<td style="padding: 10px; text-align: center;">35.98%</td>
					</tr>
					<tr style="border-bottom: 1px solid #ddd;">
						<td style="padding: 10px;"><strong>RL-Trained SD + CLIP (Null Prompt)</strong></td>
						<td style="padding: 10px; text-align: center;">14.51%</td>
						<td style="padding: 10px; text-align: center;">32.18%</td>
						<td style="padding: 10px; text-align: center;">41.07%</td>
					</tr>
					<tr>
						<td style="padding: 10px;"><strong>SFT-Trained SD + CLIP (Null Prompt)</strong></td>
						<td style="padding: 10px; text-align: center;">25.32%</td>
						<td style="padding: 10px; text-align: center;">45.95%</td>
						<td style="padding: 10px; text-align: center;">56.07%</td>
					</tr>
				</tbody>
			</table>

			<p>
				RL and SFT improve classification accuracy over an untuned SD +
				CLIP pipeline. However, a zero-shot clip baseline still
				significantly exceeds the performance of all the trained pairs. 
			</p>


			<h4>Results: SFT-Trained Model</h4>
			<p>
				This model was trained for 30 epochs using the supervised fine-tuning method described above.
				This model uses empty prompts for the diffusion model, which we discuss below to perform with
				no meaningful difference from custom prompts.
			</p>
		</div>
		<div class="margin-right-block"></div>
	</div>

	<div class="content-margin-container">
		<div class="margin-left-block"></div>
		<div class="main-content-block">
			<div style="display: flex; flex-wrap: nowrap; gap: 15px; margin: 20px 0; justify-content: center;">
				<!-- Pair 1 -->
				<div style="flex: 1; min-width: 0;">
					<div style="display: flex; gap: 8px;">
						<div style="flex: 1;">
							<img src="./images/good_no_prompt_sft_before_1.png" alt="Before: Original camouflaged image"
								style="width: 100%; border: 1px solid #ddd;" />
							<p class="image-caption-small">
								<strong>Before</strong><br />
								Grasshopper
							</p>
						</div>
						<div style="flex: 1;">
							<img src="./images/good_no_prompt_sft_after_1.png" alt="After: Enhanced image"
								style="width: 100%; border: 1px solid #ddd;" />
							<p class="image-caption-small">
								<strong>After</strong><br />
								Grasshopper
							</p>
						</div>
					</div>
				</div>

				<!-- Pair 2 -->
				<div style="flex: 1; min-width: 0;">
					<div style="display: flex; gap: 8px;">
						<div style="flex: 1;">
							<img src="./images/original_2.png" alt="Before: Original camouflaged image"
								style="width: 100%; border: 1px solid #ddd;" />
							<p class="image-caption-small">
								<strong>Before</strong><br />
								Frog
							</p>
						</div>
						<div style="flex: 1;">
							<img src="./images/no_prompt_sft_2.png" alt="After: Enhanced image"
								style="width: 100%; border: 1px solid #ddd;" />
							<p class="image-caption-small">
								<strong>After</strong><br />
								Frog
							</p>
						</div>
					</div>
				</div>

				<!-- Pair 3 -->
				<div style="flex: 1; min-width: 0;">
					<div style="display: flex; gap: 8px;">
						<div style="flex: 1;">
							<img src="./images/original_3.png" alt="Before: Original camouflaged image"
								style="width: 100%; border: 1px solid #ddd;" />
							<p class="image-caption-small">
								<strong>Before</strong><br />
								Spider
							</p>
						</div>
						<div style="flex: 1;">
							<img src="./images/no_prompt_sft_3.png" alt="After: Enhanced image"
								style="width: 100%; border: 1px solid #ddd;" />
							<p class="image-caption-small">
								<strong>After</strong><br />
								Spider
							</p>
						</div>
					</div>
				</div>
			</div>
			<p class="image-caption" style="font-size: 1.1em; font-weight: bold; margin-top: 15px; margin-bottom: 30px; color: black;">
				'Good' Generations
			</p>
		</div>
		<div class="margin-right-block"><strong>Figure 6</strong></div>
	</div>

	<div class="content-margin-container">
		<div class="margin-left-block"></div>
		<div class="main-content-block">
			<div style="display: flex; flex-wrap: nowrap; gap: 15px; margin: 20px 0; justify-content: center;">
				<!-- Failure Pair 1 -->
				<div style="flex: 1; min-width: 0;">
					<div style="display: flex; gap: 8px;">
						<div style="flex: 1;">
							<img src="./images/fail_no_prompt_sft_before_4.png" alt="Before: Original camouflaged image"
								style="width: 100%; border: 1px solid #ddd;" />
							<p class="image-caption-small">
								<strong>Before</strong><br />
								Turtle<br />
							</p>
						</div>
						<div style="flex: 1;">
							<img src="./images/fail_no_prompt_sft_after_4.png" alt="After: Enhanced image"
								style="width: 100%; border: 1px solid #ddd;" />
							<p class="image-caption-small">
								<strong>After</strong><br />
								Owl<br />
							</p>
						</div>
					</div>
				</div>

				<!-- Failure Pair 2 -->
				<div style="flex: 1; min-width: 0;">
					<div style="display: flex; gap: 8px;">
						<div style="flex: 1;">
							<img src="./images/fail_no_prompt_sft_before_5.png" alt="Before: Original camouflaged image"
								style="width: 100%; border: 1px solid #ddd;" />
							<p class="image-caption-small">
								<strong>Before</strong><br />
								Snake<br />
							</p>
						</div>
						<div style="flex: 1;">
							<img src="./images/fail_no_prompt_sft_after_5.png" alt="After: Enhanced image"
								style="width: 100%; border: 1px solid #ddd;" />
							<p class="image-caption-small">
								<strong>After</strong><br />
								Frog<br />
							</p>
						</div>
					</div>
				</div>

				<!-- Failure Pair 3 -->
				<div style="flex: 1; min-width: 0;">
					<div style="display: flex; gap: 8px;">
						<div style="flex: 1;">
							<img src="./images/fail_no_prompt_sft_before_6.png" alt="Before: Original camouflaged image"
								style="width: 100%; border: 1px solid #ddd;" />
							<p class="image-caption-small">
								<strong>Before</strong><br />
								Spider<br />
							</p>
						</div>
						<div style="flex: 1;">
							<img src="./images/fail_no_prompt_sft_after_6.png" alt="After: Enhanced image"
								style="width: 100%; border: 1px solid #ddd;" />
							<p class="image-caption-small">
								<strong>After</strong><br />
								Moth<br />
							</p>
						</div>
					</div>
				</div>
			</div>
			<p class="image-caption" style="font-size: 1.1em; font-weight: bold; margin-top: 15px; margin-bottom: 30px; color: black;">
				'Bad' Generations
			</p>
		</div>
		<div class="margin-right-block" id="figure_7"><strong>Figure 7</strong></div>
	</div>

	<div class="content-margin-container">
		<div class="margin-left-block"></div>
		<div class="main-content-block">

			<p>
				The first row shows results of "good" generations from diffusion model, while the second row shows poor
				generations that lead to misclassifications.
				The good generations show some promising behavior, such as a blurring of the background which enhances
				the camouflaged object.
				This is apparent in the frog and spider image pairs, where the background is noticeably blurred relative
				to the camouflaged object.
				The grasshopper's background is initially blurred, but the diffusion model seems to do additionally blend
				and simplify the background colors.
			</p>
			<p>
				In each image pair of the "poor" generations, the diffusion model seems to misidentify the
				camouflaged object as a different class and denoise towards that data distribution. This explains why
				the SFT-trained model performs worse than the baseline
				camouflage dataset images. This is very noticeable in the snake-frog image pair, where the snake
				totally disappears and some resemblance of a frog appears. The spider-moth image pair shows another
				interesting behavior, where a "moth" object is seemingly
				generated independently. It is worth noting that even to the human eye, the camouflage images for the
				"poor" generations are considerably harder, although
				they all belong to the same COD10K dataset without difficulty distinctions.
			</p>

		</div>
		<div class="margin-right-block"></div>
	</div>

	<div class="content-margin-container">
		<div class="margin-left-block"></div>
		<div class="main-content-block">
			<div style="display: flex; flex-wrap: wrap; gap: 20px; margin: 20px 0; justify-content: center;">
				<div style="flex: 1; min-width: 400px; max-width: 500px;">
					<img src="./images/val_loss_comparison.png"
						alt="Validation loss comparison across prompt strategies"
						style="width: 100%; border: 1px solid #ddd;" />
					<p class="image-caption">
						Validation loss comparison
					</p>
				</div>
				<div style="flex: 1; min-width: 400px; max-width: 500px;">
					<img src="./images/val_accuracy_comparison.png"
						alt="Validation accuracy comparison across prompt strategies"
						style="width: 100%; border: 1px solid #ddd;" />
					<p class="image-caption">
						Validation accuracy comparison
					</p>
				</div>
			</div>
		</div>
		<div class="margin-right-block"><strong>Figure 8</strong></div>
	</div>

	<div class="content-margin-container" id="results_prompts">
		<div class="margin-left-block"></div>
		<div class="main-content-block">

			<p>
				We also compared performance across different prompt
				strategies. Figure 8 shows validation loss and accuracy curves for models trained with
				custom prompts ("De-camouflage the animal", "Increase edge sharpness and contrast",
				"Enhance visibility of camouflaged animal") versus the no-prompt baseline. Despite
				the semantic differences in prompt wording, all custom prompts performed similarly to
				each other and showed no meaningful improvement over the no-prompt condition.		
			</p>

		</div>
		<div class="margin-right-block"></div>
	</div>

	<div class="content-margin-container">
		<div class="margin-left-block"></div>
		<div class="main-content-block">
			<div style="display: flex; flex-wrap: nowrap; gap: 15px; margin: 20px 0; justify-content: center;">
				<!-- Pair 1 -->
				<div style="flex: 1; min-width: 0;">
					<div style="display: flex; gap: 8px;">
						<div style="flex: 1;">
							<img src="./images/oracle_prompt_before_1.png" alt="Before: Original camouflaged image"
								style="width: 100%; border: 1px solid #ddd;" />
							<p class="image-caption-small">
								<strong>Before</strong><br />
								Cat
							</p>
						</div>
						<div style="flex: 1;">
							<img src="./images/oracle_prompt_after_1.png" alt="After: Enhanced image"
								style="width: 100%; border: 1px solid #ddd;" />
							<p class="image-caption-small">
								<strong>After</strong><br />
								Cat
							</p>
						</div>
					</div>
				</div>
				
				<!-- Pair 2 -->
				<div style="flex: 1; min-width: 0;">
					<div style="display: flex; gap: 8px;">
						<div style="flex: 1;">
							<img src="./images/oracle_prompt_before_2.png" alt="Before: Original camouflaged image"
								style="width: 100%; border: 1px solid #ddd;" />
							<p class="image-caption-small">
								<strong>Before</strong><br />
								Frog
							</p>
						</div>
						<div style="flex: 1;">
							<img src="./images/oracle_prompt_after_2.png" alt="After: Enhanced image"
								style="width: 100%; border: 1px solid #ddd;" />
							<p class="image-caption-small">
								<strong>After</strong><br />
								Frog
							</p>
						</div>
					</div>
				</div>
				
				<!-- Pair 3 -->
				<div style="flex: 1; min-width: 0;">
					<div style="display: flex; gap: 8px;">
						<div style="flex: 1;">
							<img src="./images/oracle_prompt_before_3.png" alt="Before: Original camouflaged image"
								style="width: 100%; border: 1px solid #ddd;" />
							<p class="image-caption-small">
								<strong>Before</strong><br />
								Mantis
							</p>
						</div>
						<div style="flex: 1;">
							<img src="./images/oracle_prompt_after_3.png" alt="After: Enhanced image"
								style="width: 100%; border: 1px solid #ddd;" />
							<p class="image-caption-small">
								<strong>After</strong><br />
								Mantis
							</p>
						</div>
					</div>
				</div>
			</div>
			<p class="image-caption" style="font-size: 1.1em; font-weight: bold; margin-top: 15px; margin-bottom: 30px; color: black;">
				ORACLE Prompt Generations
			</p>
		</div>
		<div class="margin-right-block"><strong>Figure 9</strong></div>
	</div>

	<div class="content-margin-container" id="results_oracle">
		<div class="margin-left-block"></div>
		<div class="main-content-block">

			<p>
				As expected, the ORACLE prompt performed the best, reaching close to 100% accuracy on the test set. However, the
				diffusion model tends to <i>cheat</i>, generating images
				unrecognizable from the original image 
				to minimize the classification loss for the desired class. 
			</p>
		</div>
		<div class="margin-right-block"></div>
	</div>

	<div class="content-margin-container" id="results_rl">
		<div class="margin-left-block"></div>
		<div class="main-content-block">
			<h4 id="results_rl">Results: RL-Trained Model</h4>
			<p>
				The results reported in <a href="#table_1">Table 1</a> were evaluated using the LoRA
				checkpoint from the green run in <a href="#figure_10">Figure
				10</a>. While this run ostensibly exhibited positive reward, the
				overall test performance is poor because reward is based on
				relative increase in logits for the desired class, rather than
				comparison to base CLIP accuracy.  
			</p>

			<p>
				We performed extensive hyperparameter tuning and extended
				training runs (over 100 runs total). However, for the camouflage
				task (where most of our effort was focused), we were not able to
				exceed the results presented in <a href="#table_1">Table 1</a>. 

			</p>

			<!-- TODO: I want to put these hyperparameters into a neater table,
			for RL and for SFT best runs. Would this go in the results section? -->
			<p>
				Most successful hyperparameter ranges tended to be:
				<ul>
					<li>Learning rate about 1e-4</li>
					<li>Noise strength around 0.2-0.4</li>
					<li>Guidance scale at 0 for null prompts, or 7.0
					otherwise</li>
					<li>Batch size per gradient update around 256</li>
					<li>Epochs up to 500, but reward continued climbing the
					entire time</li>
					<li>Train dataset size: validation performance improves with
					size, but reward climb is slower as the model has to learn
					more about the dataset. In most of our testing for RL we
					kept dataset size between 256 and 1028.</li>
				</ul>

			</p>

		</div>
		<div class="margin-right-block"></div>
	</div>

	<div class="content-margin-container">
		<div class="margin-left-block"></div>
		<div class="main-content-block">
			<img src="./images/rl_reward_graph.png" style="width: 80%; height: auto;" />
			<p class="heavy-caption">Comparing Reward Progress of Different
			Training Configurations</p>
		</div>
		<div class="margin-right-block"><strong>Figure 10</strong><br /><br
		/>With only a single image and ORACLE prompt, the blue run sanity-checks
		our training setup's ability to overfit. As we remove these 'training
		wheels', these runs demonstrate that reward climbing is much slower
		with larger training datasets (red run) and null prompts (green
		run).</div>
	</div>

	<div class="content-margin-container" id="results_cifar">
		<div class="margin-left-block"></div>
		<div class="main-content-block">
			<h2>Corruption Task (Preliminary)</h2>
			We began extending our pipeline to the corruption task (CIFAR-10-C and ImageNet-C) but were not able
			to obtain stable or interpretable generations within the project
			timeline. Across the two hyperparameter configurations we tested, the
			diffusion model quickly collapsed to heavily corrupted outputs and did
			not improve CLIP accuracy, so we do not report quantitative results for
			this setting.
			<br /><br />
			Nevertheless, we view CIFAR-10-C (and ImageNet-C at higher resolution)
			as more promising testbeds than COD10K for future work. Their corruption
			structure better matches the denoising bias of diffusion models and
			enables stronger reward signals via CIFAR- or ImageNet-adapted vision
			classifiers, rather than relying on an open-vocabulary CLIP model.
			In our limited testing, reward was consistently climbing at a slow
			rate of \(\sim 4e-4\) per image sampled. 
		</div>
		<div class="margin-right-block">
		</div>
	</div>

	<div class="content-margin-container" id="discussion">
		<div class="margin-left-block"></div>
		<div class="main-content-block">
			<h1>Discussion</h1>
	
			<h3>Results Overview</h3>
			<p>
				On the COD10K task, our SFT and RL-trained generative models
				achieve 25.32% and 14.51% top-1 accuracy, respectively. While
				this falls short of the 44.87% CLIP baseline, both represent a
				significant improvement over the untrained BASE SD + CLIP
				combination, with SFT more than doubling its 11.40% accuracy.
			</p>
	
			<h3 id="discussion_failure_analysis">Failure Analysis</h3>
			<p>
				The fundamental challenge stems from tasking a generative
				diffusion model with an implicitly discriminative objective.
				Diffusion models are not pretrained for semantic label
				preservation; rather, they are optimized to produce clean,
				high-fidelity images from noise. This structural circularity
				renders the task fundamentally ill-posed: the model implicitly
				requires the semantic label to correctly denoise the image, yet
				the purpose of the denoising is to recover that very label.
				
				<br><br>
				When presented with the label ambiguity inherent in camouflaged
				images, the model performs stochastic sampling over plausible
				choices. Consequently, if the model initially misidentifies the
				subject, it denoises toward the wrong class distribution,
				effectively hallucinating a high-quality but semantically
				incorrect object. This failure mode is evidenced by the 'Bad'
				generations in <a href="#figure_7">Figure 7</a>.
			</p>
			<p>
				From an optimization perspective, this approach also struggles with credit assignment.
				Propagating gradients or rewards through 10-20 denoising steps is computationally
				expensive and obscures the causal link between specific denoising actions and the
				final classification. Furthermore, the reward signal itself is sparse and exhibits
				high variance, bounded by the relatively weak performance (< 50%) 
				of the CLIP baseline on this specific dataset. This combination
				makes the optimization landscape difficult to traverse and
				highly sample-inefficient. </p>
	
					<!-- <h3>Limitations</h3>
					<p>
						The primary limitation is the accuracy-corruption tradeoff inherent in the I2I
						diffusion process. While capable of enhancing images by "de-camouflaging" subjects,
						the model risks corrupting the semantic content when it hallucinates incorrect features.
						Additionally, the reliance on a diffusion backbone introduces substantial inference
						latency due to the iterative denoising process. Our experiments are also limited by
						the scale of the filtered COD10K dataset (~6,000 images), and the LoRA adapters are
						highly specialized; generalization to other domains (e.g., medical imaging or
						satellite imagery) would likely require separate fine-tuning on task-specific data.
					</p> -->
					
					<h3 id="discussion_extensions">Extensions</h3>
					<p>
						We propose several extensions to address these limitations and improve performance:
					<ul>
						<li><strong>Hyperparameter Tuning:</strong> Systematic optimization of
							<code>noise_strength</code>, <code>learning_rate</code>, and
							<code>guidance_scale</code> (in order of expected impact).
						</li>
						<li><strong>Compute and Data:</strong> Scaling up to longer training runs and
							expanding the dataset size to improve generalization.</li>
						<li><strong>Confidence-Based Gating:</strong> Implementing a mechanism to only
							apply diffusion modifications to images where the baseline classifier has low
							confidence.</li>
						<li><strong>Annealing ORACLE Prompt:</strong> Initializing generation with a
							high <code>guidance_scale</code> using the ground-truth label (ORACLE), then
							gradually annealing the guidance to transition the model from prompt-dependence
							to autonomous detection.</li>
						<li><strong>New Tasks:</strong> Applying this generative-classification approach
							to other domains, such as classifying partial sketches by generating the complete
							scene.</li>
					</ul>
			</p>
		</div>
		<div class="margin-right-block"></div>
	</div>
	
	<div class="content-margin-container" id="conclusion">
		<div class="margin-left-block"></div>
		<div class="main-content-block">
			<h1>Conclusion</h1>
			<p>
				Our investigation yields a negative result: employing a
				diffusion prior for semantic denoising fails to improve
				zero-shot classification performance on camouflaged or corrupted
				data.
			</p>
			<p>
				While Stable Diffusion adapted with both supervised fine-tuning
				and ImageDDPO exhibited improvements over a base, unadapted
				generator, the inherent stochasticity of the diffusion process
				distracts from the final goal of improving classification. This
				results in the hallucination of new features rather than the
				pronounciation of existing ones. When weighed against the
				substantial computational overhead of iterative denoising, this
				accuracy-corruption tradeoff renders the approach ineffective
				compared to standard discriminative baselines (or more direct
				methods like SFT or linear probing on the classifier).
			</p>
			<p>
				We present these findings to discourage similar architectural
				choices in the future: for tasks requiring precise semantic
				preservation under ambiguity, current generative priors are
				ill-suited to serve as pre-processing filters.
			</p>
		</div>
		<div class="margin-right-block"></div>
	</div>

	<div class="content-margin-container" id="citations">
		<div class="margin-left-block"></div>
		<div class="main-content-block">
			<div class="citation" id="references" style="height: auto">
				<br />
				<span style="font-size: 16px">References:</span><br /><br />
				<ol class="references">
					<li id="ref_ho">
						Ho, J., Jain, A., & Abbeel, P. (2020).
						<a href="https://arxiv.org/abs/2006.11239">Denoising Diffusion Probabilistic Models</a>.
						NeurIPS 2020.
					</li>
					<li id="ref_sohldickstein">
						Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., & Ganguli, S. (2015).
						<a href="https://arxiv.org/abs/1503.03585">Deep Unsupervised Learning using Nonequilibrium Thermodynamics</a>.
						ICML 2015.
					</li>
					<li id="ref_ho_cfg">
						Ho, J., & Salimans, T. (2022).
						<a href="https://arxiv.org/abs/2207.12598">Classifier-Free Diffusion Guidance</a>.
						arXiv:2207.12598.
					</li>
					<li id="ref_brooks">
						Brooks, T., et al. (2023).
						<a href="https://arxiv.org/abs/2211.09800">InstructPix2Pix: Learning to Follow Image Editing Instructions</a>.
						arXiv:2211.09800.
					</li>
					<li id="ref_zhang">
						Zhang, L., et al. (2023).
						<a href="https://arxiv.org/abs/2302.05543">Adding Conditional Control to Text-to-Image Diffusion Models</a>.
						arXiv:2302.05543.
					</li>
					<li id="ref_rombach">
						Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2022).
						<a href="https://arxiv.org/abs/2112.10752">High-Resolution Image Synthesis with Latent Diffusion Models</a>.
						CVPR 2022.
					</li>
					<li id="ref_song">
						Song, J., Meng, C., & Ermon, S. (2021).
						<a href="https://arxiv.org/abs/2010.02502">Denoising Diffusion Implicit Models</a>.
						ICLR 2021.
					</li>
					<li id="ref_radford">
						Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., ... & Sutskever, I. (2021).
						<a href="https://arxiv.org/abs/2103.00020">Learning Transferable Visual Models From Natural Language Supervision</a>.
						ICML 2021.
					</li>
					<li id="ref_black">
						Black, K., Janner, M., Du, Y., Kostrikov, I., & Levine, S. (2023).
						<a href="https://arxiv.org/abs/2305.13301">Training Diffusion Models with Reinforcement Learning</a>.
						NeurIPS 2023.
					</li>
					<li id="ref_fan">
						Fan, D.-P., et al. (2021).
						<a href="https://doi.org/10.48550/arXiv.2102.10274">Concealed Object Detection</a>.
						arXiv:2102.10274.
					</li>
					<li id="ref_hendrycks">
						Hendrycks, D., & Dietterich, T. (2019).
						<a href="https://doi.org/10.48550/arXiv.1903.12261">Benchmarking Neural Network Robustness to Common Corruptions and Perturbations</a>.
						arXiv:1903.12261.
					</li>
					<li id="ref_pg_intro">
						Arxiv Insights (2018).
						<a href="https://www.youtube.com/watch?v=5P7I-xPq8u8&t=17s">An Introduction to Policy Gradient Methods</a>.
						YouTube.
					</li>
					<li id="ref_schulman_gae">
						Schulman, J., Moritz, P., Levine, S., Jordan, M., & Abbeel, P. (2016).
						<a href="https://doi.org/10.48550/arXiv.1506.02438">High-Dimensional Continuous Control Using Generalized Advantage Estimation</a>.
						arXiv:1506.02438.
					</li>
					<li id="ref_schulman_ppo">
						Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017).
						<a href="https://doi.org/10.48550/arXiv.1707.06347">Proximal Policy Optimization Algorithms</a>.
						arXiv:1707.06347.
					</li>
					<li id="ref_trl_ddpo">
						HuggingFace (n.d.).
						<a href="https://huggingface.co/docs/trl/main/ddpo_trainer">Denoising Diffusion Policy Optimization</a>.
						HuggingFace TRL Documentation.
					</li>
				</ol>
			</div>
		</div>
		<div class="margin-right-block"></div>
	</div>
	<script>
		document.addEventListener("DOMContentLoaded", function () {
			const refList = document.querySelector('ol.references');
			if (!refList) return;

			const refData = {};
			const refUrls = {};
			Array.from(refList.children).forEach(item => {
				if (item.id) {
					refData[item.id] = item.innerHTML;
					const link = item.querySelector('a');
					if (link) {
						refUrls[item.id] = link.getAttribute('href');
					}
				}
			});

			const citations = document.querySelectorAll('a.cite');
			let citationCount = 0;
			const citedIDs = new Map();
			const marginDisplayedIDs = new Set();
			const marginNotes = [];
			const spacing = 8;

			citations.forEach(cite => {
				const href = cite.getAttribute('href');
				if (!href || !href.startsWith('#')) return;

				const id = href.substring(1);
				if (!citedIDs.has(id)) {
					citationCount++;
					citedIDs.set(id, citationCount);
				}

				const num = citedIDs.get(id);
				cite.textContent = `[${num}]`;

				if (refUrls[id]) {
					cite.setAttribute('href', refUrls[id]);
					cite.setAttribute('target', '_blank');
				}

				if (marginDisplayedIDs.has(id)) return;
				marginDisplayedIDs.add(id);

				const container = cite.closest('.content-margin-container');
				if (!container) return;

				const rightMargin = container.querySelector('.margin-right-block');
				if (!rightMargin) return;

				const note = document.createElement('div');
				note.className = 'citation-margin';
				note.innerHTML = `<span style="font-weight: bold;">${num}</span> ${refData[id]}`;
				rightMargin.appendChild(note);
				marginNotes.push({ cite, note, container, rightMargin });
			});

			const getBaseHeight = (margin) => {
				let maxBottom = 0;
				Array.from(margin.children).forEach(child => {
					if (child.classList && child.classList.contains('citation-margin')) return;
					const bottom = child.offsetTop + child.offsetHeight;
					if (bottom > maxBottom) {
						maxBottom = bottom;
					}
				});
				return maxBottom;
			};

			const positionMarginNotes = () => {
				const perMargin = new Map();

				marginNotes.forEach(({ rightMargin }) => {
					if (!perMargin.has(rightMargin)) {
						perMargin.set(rightMargin, {
							baseHeight: getBaseHeight(rightMargin),
							lastBottom: null
						});
						if (!rightMargin.style.position) {
							rightMargin.style.position = 'relative';
						}
					}
				});

				marginNotes.forEach(({ cite, note, container, rightMargin }) => {
					const entry = perMargin.get(rightMargin);
					if (!entry || !container) return;

					const containerRect = container.getBoundingClientRect();
					const citeRect = cite.getBoundingClientRect();
					const desiredTop = citeRect.top - containerRect.top;

					const previousBottom = entry.lastBottom === null
						? entry.baseHeight + spacing
						: entry.lastBottom + spacing;
					const top = Math.max(desiredTop, previousBottom);

					note.style.position = 'absolute';
					note.style.left = '0';
					note.style.right = '0';
					note.style.width = '100%';
					note.style.top = `${top}px`;

					entry.lastBottom = top + note.offsetHeight;
					rightMargin.style.minHeight = `${Math.max(entry.lastBottom + spacing, entry.baseHeight)}px`;
				});
			};

			positionMarginNotes();
			window.addEventListener('resize', positionMarginNotes);
			window.addEventListener('load', positionMarginNotes);
		});
	</script>
</body>

</html>
