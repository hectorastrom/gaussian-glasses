<!DOCTYPE html>
<html>
  <head>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
  <head>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>

    <link rel="shortcut icon" href="images/icon.ico" />
    <style type="text/css">
          body {
          	background-color: #f5f9ff;
          }

          /* Hide both math displays initially, will display based on JS detection */
          .mathjax-mobile,
          .mathml-non-mobile {
          	display: none;
          }

          /* Show the MathML content by default on non-mobile devices */
          .show-mathml .mathml-non-mobile {
          	display: block;
          }

          .show-mathjax .mathjax-mobile {
          	display: block;
          }

          .content-margin-container {
          	display: flex;
          	width: 100%;
          	/* Ensure the container is full width */
          	justify-content: left;
          	/* Horizontally centers the children in the container */
          	align-items: center;
          	/* Vertically centers the children in the container */
          }

          .main-content-block {
          	width: 70%;
          	/* Change this percentage as needed */
          	max-width: 1100px;
          	/* Optional: Maximum width */
          	background-color: #fff;
          	border-left: 1px solid #DDD;
          	border-right: 1px solid #DDD;
          	padding: 8px 8px 8px 8px;
          	font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
          	#"Avenir";
          }

          .margin-left-block {
          	font-size: 14px;
          	width: 15%;
          	/* Change this percentage as needed */
          	max-width: 130px;
          	/* Optional: Maximum width */
          	position: relative;
          	margin-left: 10px;
          	text-align: left;
          	font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
          	#"Avenir";
          	padding: 5px;
          }

          .margin-right-block {
          	font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
          	#"Avenir";
          	font-size: 14px;
          	width: 25%;
          	/* Change this percentage as needed */
          	max-width: 256px;
          	/* Optional: Maximum width */
          	position: relative;
          	text-align: left;
          	padding: 10px;
          	/* Optional: Adds padding inside the caption */
          }

          img {
          	max-width: 100%;
          	/* Make sure it fits inside the container */
          	height: auto;
          	display: block;
          	margin: auto;
          }

          .my-video {
          	max-width: 100%;
          	/* Make sure it fits inside the container */
          	height: auto;
          	display: block;
          	margin: auto;
          }

          /* Hide both video displays initially, will display based on JS detection */
          .vid-mobile,
          .vid-non-mobile {
          	display: none;
          }

          /* Show the video content by default on non-mobile devices */
          .show-vid-mobile .vid-mobile {
          	display: block;
          }

          .show-vid-non-mobile .vid-non-mobile {
          	display: block;
          }

          a:link,
          a:visited {
          	color: #0e7862;
          	/*#1367a7;*/
          	text-decoration: none;
          }

          a:hover {
          	color: #24b597;
          	/*#208799;*/
          }

      h1 {
      	font-size: 24px;
      	margin-top: 32px;
      	margin-bottom: 8px;
      }

      h2 {
      	font-size: 18px;
      	margin-top: 12px;
      	margin-bottom: 6px;
      }

          table.header {
          	font-weight: 300;
          	font-size: 17px;
          	flex-grow: 1;
          	width: 70%;
          	max-width: calc(100% - 290px);
          	/* Adjust according to the width of .paper-code-tab */
          }

          table td,
          table td * {
          	vertical-align: middle;
          	position: relative;
          }

          table.paper-code-tab {
          	flex-shrink: 0;
          	margin-left: 8px;
          	margin-top: 8px;
          	padding: 0px 0px 0px 8px;
          	width: 290px;
          	height: 150px;
          }

          .layered-paper {
          	/* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
          	box-shadow:
          		0px 0px 1px 1px rgba(0, 0, 0, 0.35),
          		/* The top layer shadow */
          		5px 5px 0 0px #fff,
          		/* The second layer */
          		5px 5px 1px 1px rgba(0, 0, 0, 0.35),
          		/* The second layer shadow */
          		10px 10px 0 0px #fff,
          		/* The third layer */
          		10px 10px 1px 1px rgba(0, 0, 0, 0.35);
          	/* The third layer shadow */
          	margin-top: 5px;
          	margin-left: 10px;
          	margin-right: 30px;
          	margin-bottom: 5px;
          }

          hr {
          	height: 1px;
          	/* Sets the height of the line to 1 pixel */
          	border: none;
          	/* Removes the default border */
          	background-color: #DDD;
          	/* Sets the line color to black */
          }

          div.hypothesis {
          	width: 80%;
          	background-color: #EEE;
          	border: 1px solid black;
          	border-radius: 10px;
          	-moz-border-radius: 10px;
          	-webkit-border-radius: 10px;
          	font-family: Courier;
          	font-size: 18px;
          	text-align: center;
          	margin: auto;
          	padding: 16px 16px 16px 16px;
          }

          div.citation {
          	font-size: 0.8em;
          	background-color: #fff;
          	padding: 10px;
          	height: 200px;
          }

          .fade-in-inline {
          	position: absolute;
          	text-align: center;
          	margin: auto;
          	-webkit-mask-image: linear-gradient(to right,
          			transparent 0%,
          			transparent 40%,
          			black 50%,
          			black 90%,
          			transparent 100%);
          	mask-image: linear-gradient(to right,
          			transparent 0%,
          			transparent 40%,
          			black 50%,
          			black 90%,
          			transparent 100%);
          	-webkit-mask-size: 8000% 100%;
          	mask-size: 8000% 100%;
          	animation-name: sweepMask;
          	animation-duration: 4s;
          	animation-iteration-count: infinite;
          	animation-timing-function: linear;
          	animation-delay: -1s;
          }

          .fade-in2-inline {
          	animation-delay: 1s;
          }

          .inline-div {
          	position: relative;
          	display: inline-block;
          	/* Makes both the div and paragraph inline-block elements */
          	vertical-align: top;
          	/* Aligns them at the top, you can adjust this to middle, bottom, etc., based on your needs */
          	width: 50px;
          	/* Optional: Adds space between the div and the paragraph */
          }
    </style>

    <title>Diffusion Lens</title>
    <meta property="og:title" content="Diffusion Lens" />
    <meta charset="UTF-8" />
  </head>

  <body>
    <div class="content-margin-container">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <table class="header" align="left">
          <tr>
            <td colspan="4">
              <span
                style="
                  font-size: 32px;
                  font-family: 'Courier New', Courier, monospace; /* Adds fallbacks */
                "
                >Diffusion Lens</span
              >
            </td>
          </tr>
          <tr>
            <td align="left">
              <span style="font-size: 17px"
                ><a href="https://hectorastrom.com">Hector Astrom</a></span
              >
            </td>
            <td align="left">
              <span style="font-size: 17px"
                ><a href="https://kevinzhu.com">Kevin Zhu</a>
                </span
              >
            </td>
            <td align="left">
              <span style="font-size: 17px"
                ><a href="your_partner's_website"
                  >Luc Gaitskell ADD YOUR LINK</a
                ></span
              >
            </td>
          </tr>

          <tr>
            <td colspan="4" align="left">
              <span style="font-size: 18px">Final project for 6.7960, MIT</span>
            </td>
          </tr>
        </table>
      </div>
      <div class="margin-right-block"></div>
    </div>
  <body>
    <div class="content-margin-container">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <table class="header" align="left">
          <tr>
            <td colspan="4">
              <span
                style="
                  font-size: 32px;
                  font-family: 'Courier New', Courier, monospace; /* Adds fallbacks */
                "
                >Diffusion Lens</span
              >
            </td>
          </tr>
          <tr>
            <td align="left">
              <span style="font-size: 17px"
                ><a href="https://hectorastrom.com">Hector Astrom</a></span
              >
            </td>
            <td align="left">
              <span style="font-size: 17px"
                ><a href="your_partner's_website"
                  >Kevin Zhu ADD YOUR LINK</a
                ></span
              >
            </td>
            <td align="left">
              <span style="font-size: 17px"
                ><a href="your_partner's_website"
                  >Luc Gaitskell ADD YOUR LINK</a
                ></span
              >
            </td>
          </tr>

          <tr>
            <td colspan="4" align="left">
              <span style="font-size: 18px">Final project for 6.7960, MIT</span>
            </td>
          </tr>
        </table>
      </div>
      <div class="margin-right-block"></div>
    </div>

    <div class="content-margin-container" id="project_image">
      <div class="margin-left-block">
        <div style="position: fixed; max-width: inherit; top: max(20%, 120px)">
          <b style="font-size: 16px">Outline</b><br /><br />
          <a href="#intro">Introduction</a><br /><br />
          <a href="#preliminaries">Preliminaries</a><br /><br />
          <a href="#task_data">Task and Data Description</a><br /><br />
          <a href="#methods">Methods</a><br /><br />
          <a href="#results">Results</a><br /><br />
          <a href="#discussion">Discussion</a><br /><br />
          <a href="#citations">References</a><br /><br />
        </div>
      </div>
      <div class="main-content-block">
        <img src="./images/your_image_here.png" width="512px" />
      </div>
      <div class="margin-right-block">Caption for the image.</div>
    </div>

    <div class="content-margin-container" id="intro">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Introduction (Hector)</h1>
        Diffusion probabilistic models <a href="#ref_1">[1]</a> have recently
        emerged as the de facto standard for generative modeling in continuous
        domains. Motivate your project. What question are you asking. Why is it
        unanswered so far? What gap in the literature or practice are you
        filling? Why is it important? Introduce both tasks and outline structure
        of this piece.
      </div>
      <div class="margin-right-block">
        Margin note that clarifies some detail for the introduction section.
      </div>
    </div>

    <div class="content-margin-container" id="preliminaries">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Preliminaries (Kevin + Hector)</h1>
        <!-- NOTE: Merging related work into this section; just cite a lot -->
        <h2 id="preliminaries_diffusion">Diffusion (Kevin)</h2>
		<p>
			In this work, we adopt conditional diffusion probabilistic models 
			(Sohl-Dickstein et al., 2015; Ho et al., 2020), which define a distribution 
			<p(x₀ | c)> over data samples <em>x₀</em> given an associated context <em>c</em>. 
			These models operate by introducing a forward noising process 
			<em>q(x<sub>t</sub> | x<sub>t−1</sub>)</em>, a Markov chain that incrementally corrupts the data with noise across many
			timesteps. The learning task is to approximate the reverse of this corruption process.
			</p>
			
			<p>
			To achieve this, a neural network <em>μ<sub>θ</sub>(x<sub>t</sub>, c, t)</em> is trained to predict the posterior mean
			of the forward process at each timestep. Training proceeds by sampling a data pair 
			(<em>x₀</em>, <em>c</em>), selecting a timestep <em>t</em> uniformly, and generating a noisy latent 
			<em>x<sub>t</sub></em> using the forward diffusion kernel. The model minimizes the difference between
			its prediction and the true posterior mean <em>μ̃(x₀, t)</em>:
			</p>
			
			<p style="margin-left: 20px;">
			L<sub>DDPM</sub>(θ) = E[ || μ̃(x₀, t) − μ<sub>θ</sub>(x<sub>t</sub>, c, t) ||² ].
			</p> FIX THE EQUATION RENDERING
			
			<p>
			This objective essentially maximizes a variational lower bound on the data
			log-likelihood, which grounds the procedure in a principled generative modeling framework.  
			Because the forward process is fixed and analytically tractable, the model’s task is to
			estimate the reverse denoising dynamics.
			</p>
			
			<p>
			At sampling time, generation begins from Gaussian noise <em>x<sub>T</sub> ∼ N(0, I)</em>. The model then
			iteratively applies the learned reverse transitions 
			<em>p<sub>θ</sub>(x<sub>t−1</sub> | x<sub>t</sub>, c)</em>, gradually removing noise and steering the sample
			toward the data distribution. Most widely used samplers (Ho et al., 2020; Song et al., 2021) implement
			this reverse update as an isotropic Gaussian with a timestep-dependent variance:
			</p>
			
			<p style="margin-left: 20px;">
			p<sub>θ</sub>(x<sub>t−1</sub> | x<sub>t</sub>, c) = N(x<sub>t−1</sub> | μ<sub>θ</sub>(x<sub>t</sub>, c, t), σ<sub>t</sub>² I).
			</p>
			
			<p>
			By chaining these denoising steps together, diffusion models have shown the ability to generate high-quality samples using an
			iterative, noise-to-data trajectory that is fully guided by the learned model.
			</p>
			
        <h2 id="preliminaries_pg">Policy Gradient Methods (Hector)</h2>
        Explain Policy Gradient fundamentals.
        <h2 id="preliminaries_ddpo">DDPOTrainer (Hector)</h2>
        Explain DDPOTrainer
      </div>
      <div class="margin-right-block">
        Margin note for Preliminaries section.
      </div>
    </div>

    <div class="content-margin-container" id="task_data">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Task and Dataset Details (Luc)</h1>
        Details about tasks & motivations (hector will already do some of this
        in intro, but doesn't hurt to remind / go into more detail) for
        camouflauge, you could say interpretability for cifar, you could say
        leveraging the denoising capabilities of diffusion models to better
        handle corruptions (something abt how they're better for it) then
        details about the datasets used, any filtering applied, and how we
        gained access (citations)
      </div>
      <div class="margin-right-block">Margin note for Data section.</div>
    </div>

    <div class="content-margin-container" id="methods">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Methods</h1>
        <h2 id="methods_imageddpo">ImageDDPOTrainer (Hector)</h2>
        Details on the ImageDDPOTrainer; what was required to implement it;
        where it can be accessed and adapted for different tasks
        <h2 id="methods_rl">RL Methods (universal) (Hector + Luc)</h2>
        Explain the methodology for the RL attempts. Hector will write about
        general setup and structure (rl loop, reward function, how
        ImageDDPOTrainer is used) Luc will write about modifications made to
        original setup for the CIFAR-C task.
		<h2 id="methods_sft">Supervised Fine-Tuning (Kevin)</h2>
		<b>Explain the methodology for the SFT camouflage attempt. Can just briefly
			say mention we didn't SFT for CIFAR-C (or even better: do run SFT for
			CIFAR-C!) Though kevin if this is easy to set up after Luc has the dset
			it would make sense to do SFT for CIFAR-C as well, as it just makes our
			paper and results very easy to follow.</b>
			<p>
			In contrast to the reinforcement learning approach described above, we also 
			explore a supervised fine-tuning (SFT) method that directly backpropagates 
			gradients through the entire diffusion pipeline. This approach eliminates the 
			variance inherent in policy gradient methods by computing exact gradients 
			through the differentiable components of the system.
			</p>

			<p>
			Our SFT method trains a LoRA adapter on the UNet component of Stable Diffusion 
			v1.5 to optimize CLIP classification accuracy. The key insight is that the 
			entire pipeline—from the UNet's denoising predictions through the VAE decoder 
			to the frozen CLIP classifier—is differentiable, allowing us to compute 
			gradients directly with respect to the classification loss.
			</p>

			<h3>Architecture and Pipeline</h3>
			<p>
			The training pipeline follows this forward pass:
			</p>
			<p style="margin-left: 20px;">
			<em>Image → VAE Encode → Add Noise → UNet (LoRA) → Denoise → VAE Decode → CLIP → Classification Loss</em>
			</p>

			<p>
			We freeze all components except the LoRA parameters applied to the UNet's 
			attention layers. Specifically, LoRA adapters are applied to the query, key, 
			value, and output projection matrices (<code>to_q</code>, <code>to_k</code>, 
			<code>to_v</code>, <code>to_out.0</code>) in all cross-attention and 
			self-attention blocks. We use a LoRA rank of 4 and alpha of 4, resulting in 
			only a small fraction of the UNet's parameters being trainable (less than 1% of the full model). The VAE encoder and decoder, text encoder, 
			and CLIP classifier are all frozen, ensuring that gradients flow only through 
			the LoRA-adapted UNet.
			</p>

			<h3>Differentiable Denoising Loop</h3>
			<p>
			The core of our approach is a fully differentiable denoising loop implemented 
			using DDIM sampling with <em>η = 0</em> (deterministic mode). This ensures 
			that the reverse diffusion process is deterministic and maintains gradient 
			flow throughout all denoising steps. For each training sample:
			</p>

			<ol>
			<li>
				<strong>VAE Encoding:</strong> The input image is encoded into the latent 
				space using the frozen VAE encoder, producing latents <em>z₀</em>.
			</li>
			<li>
				<strong>Noise Injection:</strong> We apply image-to-image (I2I) conditioning 
				by adding noise to the latents. Given a noise strength parameter 
				<em>α</em> (typically 0.4), we select a timestep <em>t<sub>start</sub></em> 
				corresponding to the <em>(1 - α)</em> quantile of the diffusion schedule, 
				then add noise: <em>z<sub>t_start</sub> = √(1-β<sub>t</sub>) z₀ + √β<sub>t</sub> ε</em>, 
				where <em>ε ~ N(0, I)</em>.
			</li>
			<li>
				<strong>Differentiable Denoising:</strong> Starting from <em>z<sub>t_start</sub></em>, 
				we run a deterministic DDIM denoising loop for the remaining 
				<em>α × num_steps</em> steps. At each timestep <em>t</em>, the UNet 
				predicts noise <em>ε<sub>θ</sub>(z<sub>t</sub>, c, t)</em>, where <em>c</em> 
				is the text conditioning. We optionally apply classifier-free guidance (CFG) 
				by computing both unconditional and conditional predictions and combining 
				them: <em>ε<sub>pred</sub> = ε<sub>uncond</sub> + γ(ε<sub>cond</sub> - ε<sub>uncond</sub>)</em>, 
				where <em>γ</em> is the guidance scale. The DDIM step update is:
			</li>
			</ol>

			<p style="margin-left: 40px;">
			<em>z<sub>t-1</sub> = √(α<sub>t-1</sub>/α<sub>t</sub>) z<sub>t</sub> + 
			(√(1-α<sub>t-1</sub>) - √(α<sub>t-1</sub>/α<sub>t</sub>)√(1-α<sub>t</sub>)) ε<sub>pred</sub></em>
			</p>

			<p>
			All operations in this loop maintain gradient flow, allowing 
			backpropagation through the entire sequence of denoising steps.
			</p>

			<h3>Classification Loss and Training</h3>
			<p>
			After denoising, the latents are decoded back to pixel space using the frozen 
			VAE decoder, producing generated images <em>x̂</em>. These images are then 
			preprocessed for CLIP (resized to 224×224 and normalized using CLIP's 
			statistics) and passed through the frozen CLIP vision encoder to obtain 
			normalized image features <em>f<sub>img</sub></em>.
			</p>

			<p>
			We pre-compute normalized text embeddings <em>f<sub>text</sub><sup>(i)</sup></em> 
			for all classes in the dataset using prompts "An image of {class_name}" for each class. 
			Classification logits are computed as:
			</p>

			<p style="margin-left: 20px;">
			<em>logits<sub>i</sub> = exp(τ) × (f<sub>img</sub> · f<sub>text</sub><sup>(i)</sup>)</em>
			</p>

			<p>
			where <em>τ</em> is CLIP's learned temperature parameter. We minimize the 
			cross-entropy loss between these logits and the ground-truth class labels 
			using standard backpropagation.
			</p>

			<h3>Training Details</h3>
			<p>
			Training uses mixed precision (FP16 for diffusion components, FP32 for CLIP) 
			and gradient accumulation to achieve an effective batch size of 16 (batch 
			size 2 × 8 accumulation steps). We use the AdamW optimizer with a learning 
			rate of 1e-4. The diffusion process uses 20 total steps with a noise strength 
			of 0.4, meaning we denoise only the final 40% of the diffusion schedule. 
			This I2I approach allows the model to make targeted modifications to the 
			input image while preserving its overall structure.
			</p>

			<p>
			We experiment with different prompt strategies: (1) <strong>ORACLE</strong> mode, 
			where prompts use the ground-truth class label (e.g., "A clear photo of {class}"), 
			(2) custom fixed prompts, and (3) empty prompts. The ORACLE setting provides 
			an upper bound on performance by giving the model perfect semantic information, 
			while empty prompts test whether the model can learn to improve classification 
			without explicit text guidance.
			</p>

			<p>
			<strong>Note:</strong> We apply SFT only to the camouflage task (COD10K dataset). 
			While the methodology is directly applicable to CIFAR-C, we focus our SFT 
			experiments on the camouflage domain where the image-to-image nature of the 
			task aligns naturally with the I2I diffusion approach.
			</p>
		<h2 id="methods_benchmarking">Evaluation (Kevin)</h2>
		<b>Details what we compared (CLIP vs. CLIP + Diffusion RL vs. CLIP +
			Diffusion SFT) hector used eval_rl script for this, but not sure what
			you used kevin. just point to that script and basics of how it is setup
			& any special changes for each test.</b>
<p>
  To evaluate the effectiveness of our diffusion-enhanced classification approach, we compare four conditions on the COD10K test set. 
  We establish two baselines: direct zero-shot classification of original images using CLIP without diffusion processing, and classification 
  of images processed through the base (untrained) Stable Diffusion model. We then evaluate our trained models: RL-trained SD and SFT-trained SD, 
  both fine-tuned for the camouflage classification task.
</p>

<p>
  All evaluations use the same CLIP model (<code>openai/clip-vit-base-patch16</code>) 
  and preprocessing pipeline to ensure fair comparison. Images are resized to 
  224×224 using bicubic interpolation and normalized using CLIP's standard 
  statistics (mean: [0.4815, 0.4578, 0.4082], std: [0.2686, 0.2613, 0.2758]). 
  We pre-compute normalized text embeddings for all classes using prompts of the 
  form "An image of {class_name}" and compute classification logits using CLIP's 
  learned temperature-scaled cosine similarity.
</p>

<p>
  For each condition, we report top-1 through top-5 classification accuracy, 
  mean negative log-likelihood (NLL), and mean probability assigned to the 
  correct class. These metrics are computed across the entire test set and 
  optionally broken down by class for per-class analysis.
</p>

      </div>
      <div class="margin-right-block">Margin note for Methods section.</div>
    </div>

    <div class="content-margin-container" id="results">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Results (All)</h1>
        TOP 1, TOP 3, and TOP 5 accuracy table for:

        <h2>Camouflage Task (Kevin)</h2>
        <p>
          Table 1 shows classification accuracy results on the COD10K test set across 
          four experimental conditions: CLIP baseline, BASE SD + CLIP, RL-trained SD + CLIP, 
          and SFT-trained SD + CLIP.
        </p>
        
        <table style="border-collapse: collapse; width: 100%; margin: 20px 0;">
          <thead>
            <tr style="border-bottom: 2px solid #333;">
              <th style="padding: 10px; text-align: left;">Method</th>
              <th style="padding: 10px; text-align: center;">Top-1 Accuracy</th>
              <th style="padding: 10px; text-align: center;">Top-3 Accuracy</th>
              <th style="padding: 10px; text-align: center;">Top-5 Accuracy</th>
            </tr>
          </thead>
          <tbody>
            <tr style="border-bottom: 1px solid #ddd;">
              <td style="padding: 10px;"><strong>CLIP Baseline</strong></td>
              <td style="padding: 10px; text-align: center;">XX.X%</td>
              <td style="padding: 10px; text-align: center;">XX.X%</td>
              <td style="padding: 10px; text-align: center;">XX.X%</td>
            </tr>
            <tr style="border-bottom: 1px solid #ddd;">
              <td style="padding: 10px;"><strong>BASE SD + CLIP</strong></td>
              <td style="padding: 10px; text-align: center;">XX.X%</td>
              <td style="padding: 10px; text-align: center;">XX.X%</td>
              <td style="padding: 10px; text-align: center;">XX.X%</td>
            </tr>
            <tr style="border-bottom: 1px solid #ddd;">
              <td style="padding: 10px;"><strong>RL-Trained SD + CLIP</strong></td>
              <td style="padding: 10px; text-align: center;">XX.X%</td>
              <td style="padding: 10px; text-align: center;">XX.X%</td>
              <td style="padding: 10px; text-align: center;">XX.X%</td>
            </tr>
            <tr>
              <td style="padding: 10px;"><strong>SFT-Trained SD + CLIP</strong></td>
              <td style="padding: 10px; text-align: center;">XX.X%</td>
              <td style="padding: 10px; text-align: center;">XX.X%</td>
              <td style="padding: 10px; text-align: center;">XX.X%</td>
            </tr>
          </tbody>
        </table>
        
        <p>
          [Add brief analysis here: e.g., "Both RL and SFT training improve classification 
          accuracy over the CLIP baseline, with SFT achieving XX.X% top-1 accuracy compared 
          to XX.X% for CLIP alone. The BASE SD baseline shows that even untrained diffusion 
          processing provides some benefit, suggesting the denoising process has an inherent 
          regularizing effect."]
        </p>
        
        <h3>Example Results</h3>
        <p>
          Figure 1 shows example images demonstrating the effect of diffusion-based enhancement 
          on camouflaged object classification. Each pair shows the original image (left) and 
          the same image after processing through the SFT-trained diffusion model (right).
        </p>
        
        <div style="display: flex; flex-wrap: wrap; gap: 15px; margin: 20px 0; justify-content: center;">
          <!-- Pair 1 -->
          <div style="flex: 0 0 auto; width: 280px;">
            <div style="display: flex; gap: 8px;">
              <div style="flex: 1;">
                <img src="./images/original_1.png" alt="Before: Original camouflaged image" style="width: 100%; border: 1px solid #ddd;" />
                <p style="text-align: center; font-size: 0.85em; margin-top: 5px; color: #666;">
                  <strong>Before</strong><br/>
                  Crocodile
                </p>
              </div>
              <div style="flex: 1;">
                <img src="./images/no_prompt_sft_1.png" alt="After: Enhanced image" style="width: 100%; border: 1px solid #ddd;" />
                <p style="text-align: center; font-size: 0.85em; margin-top: 5px; color: #666;">
                  <strong>After</strong><br/>
                  Crocodile
                </p>
              </div>
            </div>
          </div>
          
          <!-- Pair 2 -->
          <div style="flex: 0 0 auto; width: 280px;">
            <div style="display: flex; gap: 8px;">
              <div style="flex: 1;">
                <img src="./images/original_2.png" alt="Before: Original camouflaged image" style="width: 100%; border: 1px solid #ddd;" />
                <p style="text-align: center; font-size: 0.85em; margin-top: 5px; color: #666;">
                  <strong>Before</strong><br/>
                  Frog
                </p>
              </div>
              <div style="flex: 1;">
                <img src="./images/no_prompt_sft_2.png" alt="After: Enhanced image" style="width: 100%; border: 1px solid #ddd;" />
                <p style="text-align: center; font-size: 0.85em; margin-top: 5px; color: #666;">
                  <strong>After</strong><br/>
                  Frog
                </p>
              </div>
            </div>
          </div>
          
          <!-- Pair 3 (template) -->
          <div style="flex: 0 0 auto; width: 280px;">
            <div style="display: flex; gap: 8px;">
              <div style="flex: 1;">
                <img src="./images/original_3.png" alt="Before: Original camouflaged image" style="width: 100%; border: 1px solid #ddd;" />
                <p style="text-align: center; font-size: 0.85em; margin-top: 5px; color: #666;">
                  <strong>Before</strong><br/>
				  Spider
                </p>
              </div>
              <div style="flex: 1;">
                <img src="./images/no_prompt_sft_3.png" alt="After: Enhanced image" style="width: 100%; border: 1px solid #ddd;" />
                <p style="text-align: center; font-size: 0.85em; margin-top: 5px; color: #666;">
                  <strong>After</strong><br/>
                  Spider
                </p>
              </div>
            </div>
          </div>
        </div>
        
        <p style="font-size: 0.9em; color: #666; font-style: italic; margin-top: 10px;">
          [Optional: Add more example pairs or a note about what visual changes the diffusion 
          model makes, e.g., "The diffusion model enhances edge features and increases contrast 
          in regions where the camouflaged object blends with the background."]
        </p>
		

        <h2>CIFAR-C Task (Luc or Hector - whoever trains)</h2>
        Comparing clip vs. clip+diffusion RL for CIFAR-C
      </div>
      <div class="margin-right-block">
        A caption for the results could go here.
      </div>
    </div>

    <div class="content-margin-container" id="discussion">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
		<h1>Discussion</h1>
		<p>
			This work demonstrates that generative priors can meaningfully improve 
  classification where discriminative models alone struggle. The "diffusion lens" 
  idea of using learned diffusion transformations to enhance images for machine 
  perception suggests foundation models can be composed as complementary modules 
  for specific downstream tasks. LoRA adaptation makes this practical, requiring 
  only a small number of trainable parameters to specialize the diffusion model.
		</p>

		<h2>Limitations</h2>
		<p>
		  There are some limitations to this work. First, using a diffusion 
		  model as a preprocessing step adds substantial latency compared to direct 
		  classification. Even with only 10-20 denoising steps, inference is 
		  significantly slower than CLIP alone, limiting deployment in certain 
		  applications. Second, our experiments focus on COD10K (~6,000 training images), 
		  and effectiveness on larger-scale tasks with hundreds of classes remains to be 
		  validated. Third, the LoRA adapters are trained specifically for camouflage 
		  detection, so generalization to other domains (medical imaging, satellite 
		  imagery) would require separate fine-tuning. Finally, the I2I process modifies 
		  input images, potentially introducing artifacts. The noise strength 
		  hyperparameter controls the enhancement-distortion tradeoff, requiring careful 
		  tuning for each application.
		</p>
		
		<h2>Conclusion and Future Work</h2>
		<p>
		  We have presented two approaches: 1) RL via DDPO and 2) supervised fine-tuning via 
		  direct gradient descent for training Stable Diffusion to enhance images for 
		  CLIP classification on challenging visual domains. Both approaches improve 
		  classification accuracy on camouflaged objects, demonstrating that generative 
		  models can serve as effective "lenses" that reveal features hidden from discriminative models. Future work could explore distilling the diffusion 
		  model into a lightweight network to reduce inference latency, using iterative 
		  refinement with predicted labels as prompts, analyzing what visual changes the model makes for interpretability, and evaluating whether improvements persist with larger CLIP variants or more capable diffusion models. We hope this work encourages further exploration of generative-discriminative model composition for challenging perception tasks.
		</p> 
      </div>
      <div class="margin-right-block"></div>
    </div>

    <div class="content-margin-container" id="citations">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <div class="citation" id="references" style="height: auto">
          <br />
          <span style="font-size: 16px">References:</span><br /><br />
          <a id="ref_1"></a>[1]
          <a href="https://en.wikipedia.org/wiki/Allegory_of_the_cave"
            >Allegory of the Cave</a
          >, Plato, c. 375 BC<br /><br />
          <a id="ref_2"></a>[2] <a href="">A Human-Level AGI</a>, OpenAI,
          2025<br /><br />
        </div>
      </div>
      <div class="margin-right-block"></div>
    </div>
  </body>
    <div class="content-margin-container" id="citations">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <div class="citation" id="references" style="height: auto">
          <br />
          <span style="font-size: 16px">References:</span><br /><br />
          <a id="ref_1"></a>[1]
          <a href="https://en.wikipedia.org/wiki/Allegory_of_the_cave"
            >Allegory of the Cave</a
          >, Plato, c. 375 BC<br /><br />
          <a id="ref_2"></a>[2] <a href="">A Human-Level AGI</a>, OpenAI,
          2025<br /><br />
        </div>
      </div>
      <div class="margin-right-block"></div>
    </div>
  </body>
</html>

