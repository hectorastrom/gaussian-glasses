<!DOCTYPE html>
<html>

<head>
	<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
	<!-- Add MathJax script for equation rendering -->
	<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

	<link rel="shortcut icon" href="images/icon.ico" />
	<style type="text/css">
		@font-face {
			font-family: 'Iowan Old Style';
			src: url('fonts/iowanoldst.ttf') format('truetype');
			font-weight: normal;
			font-style: normal;
		}

		@font-face {
			font-family: 'Iowan Old Style';
			src: url('fonts/iowanoldst-italic.ttf') format('truetype');
			font-weight: normal;
			font-style: italic;
		}

		@font-face {
			font-family: 'Iowan Old Style';
			src: url('fonts/iowanoldst-bold-italic.ttf') format('truetype');
			font-weight: bold;
			font-style: italic;
		}

		@font-face {
			font-family: 'Caudex-Bold';
			src: url('fonts/Caudex-Bold.ttf') format('truetype');
			font-weight: bold;
			font-style: normal;
		}

		body {
			background-color: #f5f9ff;
		}

		/* Hide both math displays initially, will display based on JS detection */
		.mathjax-mobile,
		.mathml-non-mobile {
			display: none;
		}

		/* Show the MathML content by default on non-mobile devices */
		.show-mathml .mathml-non-mobile {
			display: block;
		}

		.show-mathjax .mathjax-mobile {
			display: block;
		}

		.content-margin-container {
			display: flex;
			width: 100%;
			/* Ensure the container is full width */
			justify-content: left;
			/* Horizontally centers the children in the container */
			align-items: flex-start;
			/* Vertically centers the children in the container */
		}

		.main-content-block {
			width: 70%;
			/* Change this percentage as needed */
			max-width: 1100px;
			/* Optional: Maximum width */
			background-color: #fff;
			border-left: 1px solid #DDD;
			border-right: 1px solid #DDD;
			padding: 8px 8px 8px 8px;
			font-family: 'Iowan Old Style', serif;
		}

		.margin-left-block {
			font-size: 14px;
			width: 15%;
			/* Change this percentage as needed */
			max-width: 130px;
			/* Optional: Maximum width */
			position: relative;
			margin-left: 10px;
			text-align: left;
			font-family: 'Iowan Old Style', serif;
			padding: 5px;
		}

		.margin-right-block {
			font-family: 'Iowan Old Style', serif;
			font-size: 14px;
			width: 25%;
			/* Change this percentage as needed */
			max-width: 256px;
			/* Optional: Maximum width */
			position: relative;
			text-align: left; 
			padding-left: 10px;
			padding-right: 5px;
			/* top right bottom left */
		}

		.citation-margin {
			width: 100%;
			font-size: 12px;
			color: #555;
			padding: 5px 0;
			box-sizing: border-box;
			margin-bottom: 10px; /* Space between stacked citations */
		}

		.citation-margin a {
			color: #555;
			text-decoration: underline;
		}

		.citation-margin a:hover {
			color: #000;
		}

		/* Hide the bottom references section but keep it for data */
		#citations {
			display: none;
		}

		img {
			max-width: 100%;
			/* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
		}

		.my-video {
			max-width: 100%;
			/* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
		}

		/* Hide both video displays initially, will display based on JS detection */
		.vid-mobile,
		.vid-non-mobile {
			display: none;
		}

		/* Show the video content by default on non-mobile devices */
		.show-vid-mobile .vid-mobile {
			display: block;
		}

		.show-vid-non-mobile .vid-non-mobile {
			display: block;
		}

		a:link,
		a:visited {
			color: #0e7862;
			/*#1367a7;*/
			text-decoration: none;
		}

		a:hover {
			color: #24b597;
			/*#208799;*/
		}

		h1 {
			font-size: 24px;
			margin-top: 32px;
			margin-bottom: 8px;
		}

		h2 {
			font-size: 18px;
			margin-top: 12px;
			margin-bottom: 6px;
		}

		table.header {
			font-weight: 300;
			font-size: 17px;
			flex-grow: 1;
			width: 70%;
			max-width: calc(100% - 290px);
			/* Adjust according to the width of .paper-code-tab */
		}

		table td,
		table td * {
			vertical-align: middle;
			position: relative;
		}

		table.paper-code-tab {
			flex-shrink: 0;
			margin-left: 8px;
			margin-top: 8px;
			padding: 0px 0px 0px 8px;
			width: 290px;
			height: 150px;
		}

		.layered-paper {
			/* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
			box-shadow:
				0px 0px 1px 1px rgba(0, 0, 0, 0.35),
				/* The top layer shadow */
				5px 5px 0 0px #fff,
				/* The second layer */
				5px 5px 1px 1px rgba(0, 0, 0, 0.35),
				/* The second layer shadow */
				10px 10px 0 0px #fff,
				/* The third layer */
				10px 10px 1px 1px rgba(0, 0, 0, 0.35);
			/* The third layer shadow */
			margin-top: 5px;
			margin-left: 10px;
			margin-right: 30px;
			margin-bottom: 5px;
		}

		hr {
			height: 1px;
			/* Sets the height of the line to 1 pixel */
			border: none;
			/* Removes the default border */
			background-color: #DDD;
			/* Sets the line color to black */
		}

		div.hypothesis {
			width: 80%;
			background-color: #EEE;
			border: 1px solid black;
			border-radius: 10px;
			-moz-border-radius: 10px;
			-webkit-border-radius: 10px;
			font-family: Courier;
			font-size: 18px;
			text-align: center;
			margin: auto;
			padding: 16px 16px 16px 16px;
		}

		div.citation {
			font-size: 0.8em;
			background-color: #fff;
			padding: 10px;
			height: 200px;
		}

		.fade-in-inline {
			position: absolute;
			text-align: center;
			margin: auto;
			-webkit-mask-image: linear-gradient(to right,
					transparent 0%,
					transparent 40%,
					black 50%,
					black 90%,
					transparent 100%);
			mask-image: linear-gradient(to right,
					transparent 0%,
					transparent 40%,
					black 50%,
					black 90%,
					transparent 100%);
			-webkit-mask-size: 8000% 100%;
			mask-size: 8000% 100%;
			animation-name: sweepMask;
			animation-duration: 4s;
			animation-iteration-count: infinite;
			animation-timing-function: linear;
			animation-delay: -1s;
		}

		.fade-in2-inline {
			animation-delay: 1s;
		}

		.inline-div {
			position: relative;
			display: inline-block;
			/* Makes both the div and paragraph inline-block elements */
			vertical-align: top;
			/* Aligns them at the top, you can adjust this to middle, bottom, etc., based on your needs */
			width: 50px;
			/* Optional: Adds space between the div and the paragraph */
		}

		.image-caption {
			text-align: center;
			font-size: 0.9em;
			margin-top: 5px;
			color: #666;
		}

		.image-caption-small {
			text-align: center;
			font-size: 0.85em;
			margin-top: 5px;
			color: #666;
		}

		ol.references {
			list-style-type: none;
			padding-left: 0;
			counter-reset: ref-counter;
		}

		ol.references li {
			counter-increment: ref-counter;
			margin-bottom: 10px;
			position: relative;
			padding-left: 30px;
		}

		ol.references li::before {
			content: "[" counter(ref-counter) "] ";
			position: absolute;
			left: 0;
			/* font-weight: bold; */
		}
	</style>

	<title>Gaussian Glasses</title>
	<meta property="og:title" content="Gaussian Glasses" />
	<meta charset="UTF-8" />
</head>

<body>
	<div class="content-margin-container">
		<div class="margin-left-block"></div>
		<div class="main-content-block">
			<table class="header" align="left">
				<tr>
					<td colspan="6">
						<span style="font-size: 32px; font-family: 'Caudex-Bold', serif">
				  			Gaussian Glasses: Do Diffusion Priors Help Classification
				  			Under Corruption and Camouflage?
						</span>
					</td>
				</tr>
				<tr>
					<td align="left">
						<span style="font-size: 17px"><a href="https://hectorastrom.com">Hector Astrom</a></span>
					</td>
					<td align="left">
						<span style="font-size: 17px"><a href="https://kevinbzhu.com">Kevin Zhu</a>
						</span>
					</td>
					<td align="left">
						<span style="font-size: 17px"><a href="your_partner's_website">Luc Gaitskell ADD YOUR
								LINK</a></span>
					</td>
				</tr>

				<tr>
					<td colspan="4" align="left">
						<span style="font-size: 18px">Final project for 6.7960, MIT</span>
					</td>
				</tr>
			</table>
		</div>
		<div class="margin-right-block"></div>
	</div>

	<div class="content-margin-container" id="project_image">
		<div class="margin-left-block">
			<div style="position: fixed; max-width: inherit; top: max(20%, 120px)">
				<b style="font-size: 16px">Outline</b><br /><br />
				<a href="#intro">Introduction</a><br /><br />
				<a href="#preliminaries">Preliminaries</a><br /><br />
				<a href="#task_data">Task and Data Description</a><br /><br />
				<a href="#methods">Methods</a><br /><br />
				<a href="#results">Results</a><br /><br />
				<a href="#discussion">Discussion</a><br /><br />
				<a href="#citations">References</a><br /><br />
			</div>
		</div>
		<div class="main-content-block">
			<img src="./images/train_pipelines.png" style="width: 80%; height: auto;" />
			<p class="image-caption">SFT and RL training pipelines</p>
		</div>
		<div class="margin-right-block">In the SFT pipeline, gradient updates are free to backpropagate through a
		frozen classifier. Conversely, in RL parameter updates are faciliated
		through DDPO, where the reward is a weighted combination of classifier
		confidence and perceptual drift. </div>
	</div>

	<div class="content-margin-container" id="intro">
		<div class="margin-left-block"></div>
		<div class="main-content-block">
			<h1>Introduction (Hector)</h1>
			Since the emergence of Denoising Diffusion Probabilistic Models
			(DDPMs), generative vision has evolved from simple synthesis to
			precise, instructional editing <a href="#ref_ho" class="cite"></a> <a href="#ref_ho_cfg" class="cite"></a>. Models like
			InstructPix2Pix <a href="#ref_brooks" class="cite"></a> and
			ControlNet <a href="#ref_zhang" class="cite"></a> have demonstrated that diffusion
			priors can manipulate
			image semantics with remarkable fidelity, effectively
			"understanding" the contents of a scene to modify it.
			<br><br>
			
			This progress motivates a central question in robust computer
			vision: <b>can a diffusion prior be used as a label-preserving
			semantic denoiser for recognition?</b> Concretely, we ask whether
			diffusion-based reconstruction can restore discriminative structure
			in ambiguous or corrupted inputs, improving downstream
			classification by transforming the <i>data</i> rather than modifying
			the classifier.
			<br><br>
			
			In this work, we investigate whether Stable Diffusion v1.5 <a href="#ref_rombach" class="cite"></a>,
			optimized through both Supervised Fine-Tuning (SFT) and
			Reinforcement Learning (DDPO <a href="#ref_black" class="cite"></a>), can improve the zero-shot
			classification performance of CLIP <a href="#ref_radford" class="cite"></a> on two distinct domains of visual
			degradation:
			<ul>
				<li>
					<b>Natural Ambiguity:</b> The COD10K (Camouflaged Object Detection) dataset <a href="#ref_fan" class="cite"></a>,
					where the signal (animal) is hidden or camouflaged.
				</li>
				<li>
					<b>Algorithmic Corruption:</b> The CIFAR-10-C dataset <a href="#ref_hendrycks" class="cite"></a>, representing
					algorithmic noise and blur degradations to CIFAR-10 images.
				</li>
			</ul>
			
			We report a negative result. Our attempts find that neither SFT nor
			RL-based optimization yield a statistically significant improvement
			over the base CLIP accuracy. 

			To read more about our hypotheses for the failure modes, refer to the <a href="#discussion">discussion.</a>
			
			<br><br>
			Given the substantial computational complexity and latency
			introduced by the diffusion loop, we conclude that generative
			semantic completion -- at least for these tasks -- is
			an ineffective approach for improving robustness to visually
			degraded inputs compared to standard classifier baselines.

			<br><br>
			All code for this experiement is available <a
			href="https://github.com/hectorastrom/diffusion-lens">here.</a>
			For readers to build on this work, we also provide a standalone implementation of the ImageDDPOTrainer
			<a href="https://github.com/hectorastrom/imageddpo">here.</a>
		</div>
		<div class="margin-right-block">
		</div>
	</div>

	<div class="content-margin-container" id="preliminaries">
		<div class="margin-left-block"></div>
		<div class="main-content-block">
			<h1>Preliminaries (Kevin + Hector)</h1>
			<!-- NOTE: Merging related work into this section; just cite a lot -->
			<h2 id="preliminaries_diffusion">Diffusion (Kevin)</h2>
			<p>
			In this work, we adopt conditional diffusion probabilistic models
			<a href="#ref_sohldickstein" class="cite"></a> <a href="#ref_ho" class="cite"></a>, which define a distribution
				\( p(x_0 | c) \) over data samples \( x_0 \) given an associated context \( c \).
				These models operate by introducing a forward noising process
				\( q(x_t | x_{t-1}) \), a Markov chain that incrementally corrupts the data with noise across many
				timesteps. The learning task is to approximate the reverse of this corruption process.
			</p>

			<p>
				To achieve this, a neural network \( \mu_\theta(x_t, c, t) \) is trained to predict the posterior mean
				of the forward process at each timestep. Training proceeds by sampling a data pair
				\( (x_0, c) \), selecting a timestep \( t \) uniformly, and generating a noisy latent
				\( x_t \) using the forward diffusion kernel. The model minimizes the difference between
				its prediction and the true posterior mean \( \tilde{\mu}(x_0, t) \):
			</p>

			$$ L_{\text{DDPM}}(\theta) = \mathbb{E} \left[ \| \tilde{\mu}(x_0, t) - \mu_\theta(x_t, c, t) \|^2 \right].
			$$

			<p>
				This objective essentially maximizes a variational lower bound on the data
				log-likelihood, which grounds the procedure in a principled generative modeling framework.
				Because the forward process is fixed and analytically tractable, the model’s task is to
				estimate the reverse denoising dynamics.
			</p>

			<p>
				At sampling time, generation begins from Gaussian noise \( x_T \sim \mathcal{N}(0, I) \). The model then
				iteratively applies the learned reverse transitions
				\( p_\theta(x_{t-1} | x_t, c) \), gradually removing noise and steering the sample
				toward the data distribution. Most widely used samplers <a href="#ref_ho" class="cite"></a> <a href="#ref_song" class="cite"></a> implement
				this reverse update as an isotropic Gaussian with a timestep-dependent variance:
			</p>

			$$ p_\theta(x_{t-1} | x_t, c) = \mathcal{N}(x_{t-1} | \mu_\theta(x_t, c, t), \sigma_t^2 I). $$

			<p>
				By chaining these denoising steps together, diffusion models have shown the ability to generate
				high-quality samples using an
				iterative, noise-to-data trajectory that is fully guided by the learned model.
			</p>

			<h2 id="preliminaries_pg">Policy Gradient Methods (Hector)</h2>
			MOTIVATE WHY WE MIGHT WANT TO USE RL TO BEGIN WTIH
			(BLACK BOX OPTIMIZATION)
			Explain Policy Gradient fundamentals.
			<h2 id="preliminaries_ddpo">DDPOTrainer (Hector)</h2>
			Explain DDPOTrainer
		</div>
		<div class="margin-right-block">
		</div>
	</div>

	<div class="content-margin-container" id="task_data">
		<div class="margin-left-block"></div>
		<div class="main-content-block">
			<h1>Task and Dataset Details (Luc)</h1>
			Details about tasks & motivations (hector will already do some of this
			in intro, but doesn't hurt to remind / go into more detail) for
			camouflauge, you could say interpretability for cifar, you could say
			leveraging the denoising capabilities of diffusion models to better
			handle corruptions (something abt how they're better for it) then
			details about the datasets used, any filtering applied, and how we
			gained access (citations)
		</div>
		<div class="margin-right-block"></div>
	</div>

	<div class="content-margin-container" id="methods">
		<div class="margin-left-block"></div>
		<div class="main-content-block">
			<h1>Methods</h1>
			<h2 id="methods_imageddpo">ImageDDPOTrainer (Hector)</h2>
			Details on the ImageDDPOTrainer; what was required to implement it;
			where it can be accessed and adapted for different tasks
			<h2 id="methods_rl">RL Methods (universal) (Hector + Luc)</h2>
			Explain the methodology for the RL attempts. Hector will write about
			general setup and structure (rl loop, reward function, how
			ImageDDPOTrainer is used) Luc will write about modifications made to
			original setup for the CIFAR-C task.
			<h2 id="methods_sft">Supervised Fine-Tuning (Kevin)</h2>
			<p>
				We also
				explore a supervised fine-tuning (SFT) method that directly backpropagates
				gradients through the entire diffusion pipeline. This approach eliminates the
				variance inherent in policy gradient methods by computing exact gradients
				through the differentiable components of the system.
			</p>

			<p>
			Our SFT method trains a LoRA adapter on the UNet component of Stable Diffusion
			<a href="#ref_rombach" class="cite"></a> to optimize CLIP <a href="#ref_radford" class="cite"></a> classification accuracy. The key insight is that the
				entire pipeline (from the UNet's denoising predictions through the VAE decoder
				to the frozen CLIP classifier) is differentiable, allowing us to compute
				gradients directly with respect to the classification loss.
			</p>

			<h3>Architecture and Pipeline</h3>
			<p>
				The training pipeline follows this forward pass:
			</p>

			$$
			\begin{gathered}
			\text{Image} \to \text{VAE Encode} \to \text{Add Noise} \to \text{UNet (LoRA)}
			\to \text{Denoise} \to \text{VAE Decode} \to \text{CLIP} \to \text{Classification Loss}
			\end{gathered}
			$$

			<p>
				We freeze all components except the LoRA parameters applied to the UNet's
				attention layers. Specifically, LoRA adapters are applied to the query, key,
				value, and output projection matrices (<code>to_q</code>, <code>to_k</code>,
				<code>to_v</code>, <code>to_out.0</code>) in all cross-attention and
				self-attention blocks. We use a LoRA rank of 4 and alpha of 4, resulting in
				only a small fraction of the UNet's parameters being trainable (less than 1% of the full model). The VAE
				encoder and decoder, text encoder,
				and CLIP classifier are all frozen, ensuring that gradients flow only through
				the LoRA-adapted UNet.
			</p>

			<h3>Differentiable Denoising Loop</h3>
			<p>
				The core of our approach is a fully differentiable denoising loop implemented
				using DDIM sampling with \( \eta = 0 \) (deterministic mode). This ensures
				that the reverse diffusion process is deterministic and maintains gradient
				flow throughout all denoising steps. For each training sample:
			</p>

			<ol>
				<li>
					<strong>VAE Encoding:</strong> The input image is encoded into the latent
					space using the frozen VAE encoder, producing latents \( z_0 \).
				</li>
				<li>
					<strong>Noise Injection:</strong> We apply image-to-image (I2I) conditioning
					by adding noise to the latents. Given a noise strength parameter
					\( \alpha \) (typically 0.4), we select a timestep \( t_{\text{start}} \)
					corresponding to the \( (1 - \alpha) \) quantile of the diffusion schedule,
					then add noise: \( z_{t_{\text{start}}} = \sqrt{1-\beta_t} z_0 + \sqrt{\beta_t} \epsilon \),
					where \( \epsilon \sim \mathcal{N}(0, I) \).
				</li>
				<li>
					<strong>Differentiable Denoising:</strong> Starting from \( z_{t_{\text{start}}} \),
					we run a deterministic DDIM denoising loop for the remaining
					\( \alpha \times \text{num_steps} \) steps. At each timestep \( t \), the UNet
					predicts noise \( \epsilon_\theta(z_t, c, t) \), where \( c \)
					is the text conditioning. We optionally apply classifier-free guidance (CFG)
					by computing both unconditional and conditional predictions and combining
					them: \( \epsilon_{\text{pred}} = \epsilon_{\text{uncond}} + \gamma(\epsilon_{\text{cond}} -
					\epsilon_{\text{uncond}}) \),
					where \( \gamma \) is the guidance scale. The DDIM step update is:
				</li>
			</ol>

			$$ z_{t-1} = \sqrt{\frac{\alpha_{t-1}}{\alpha_t}} z_t + \left( \sqrt{1-\alpha_{t-1}} -
			\sqrt{\frac{\alpha_{t-1}}{\alpha_t}} \sqrt{1-\alpha_t} \right) \epsilon_{\text{pred}} $$

			<p>
				All operations in this loop maintain gradient flow, allowing
				backpropagation through the entire sequence of denoising steps.
			</p>

			<h3>Classification Loss and Training</h3>
			<p>
				After denoising, the latents are decoded back to pixel space using the frozen
				VAE decoder, producing generated images \( \hat{x} \). These images are then
				preprocessed for CLIP (resized to 224×224 and normalized using CLIP's
				statistics) and passed through the frozen CLIP vision encoder to obtain
				normalized image features \( f_{\text{img}} \).
			</p>

			<p>
				We pre-compute normalized text embeddings \( f_{\text{text}}^{(i)} \)
				for all classes in the dataset using prompts "An image of {class_name}" for each class.
				Classification logits are computed as:
			</p>

			$$ \text{logits}_i = \exp(\tau) \times (f_{\text{img}} \cdot f_{\text{text}}^{(i)}) $$

			<p>
				where \( \tau \) is CLIP's learned temperature parameter. We minimize the
				cross-entropy loss between these logits and the ground-truth class labels
				using standard backpropagation.
			</p>

			<h3>Experiments</h3>
			<p>
				Training uses mixed precision (FP16 for diffusion components, FP32 for CLIP)
				and gradient accumulation to achieve an effective batch size of 16 (batch
				size 2 × 8 accumulation steps). We use the AdamW optimizer with a learning
				rate of 1e-4. The diffusion process uses 20 total steps with a noise strength
				of 0.4, meaning we denoise only the final 40% of the diffusion schedule.
				This I2I approach allows the model to make targeted modifications to the
				input image while preserving its overall structure.
			</p>

			<p>
				We experiment with different prompt strategies: (1) ORACLE mode,
				where prompts use the ground-truth class label (e.g., "A clear photo of {class}"),
				(2) custom fixed prompts, and (3) empty prompts. The ORACLE setting provides
				an upper bound on performance by giving the model perfect semantic information,
				while empty prompts test whether the model can learn to improve classification
				without explicit text guidance.
			</p>

			<p>
				For the custom fixed prompts, we use the following prompts:
			<ul>
				<li>"De-camouflage the animal" — a direct semantic instruction to remove camouflage effects, testing
					whether explicit task guidance helps preserve the correct object.</li>
				<li>"Increase edge sharpness and contrast of the camouflaged animal" — focuses on low-level visual
					features rather than high-level semantics, testing whether edge enhancement alone improves
					classification.</li>
				<li>"Enhance the visibility of the camouflaged animal" — a general instruction that leaves the method of
					enhancement unspecified, testing whether vague guidance is sufficient.</li>
			</ul>
			</p>

			<h2 id="methods_benchmarking">Evaluation</h2>
			<p>
				To evaluate the effectiveness of our diffusion-enhanced classification approach, we compare four
				conditions on the COD10K test set.
				We establish two baselines: direct zero-shot classification of original images using CLIP without
				diffusion processing, and classification
				of images processed through the base (untrained) Stable Diffusion model. We then evaluate our trained
				models: RL-trained SD and SFT-trained SD,
				both fine-tuned for the camouflage classification task.
			</p>

			<p>
				All evaluations use the same CLIP model (<code>openai/clip-vit-base-patch16</code>)
				and preprocessing pipeline to ensure fair comparison. Images are resized to
				224×224 using bicubic interpolation and normalized using CLIP's standard
				statistics (mean: [0.4815, 0.4578, 0.4082], std: [0.2686, 0.2613, 0.2758]).
				We pre-compute normalized text embeddings for all classes using prompts of the
				form "An image of {class_name}" and compute classification logits using CLIP's
				learned temperature-scaled cosine similarity.
			</p>

			<p>
				For each condition, we report top-1 through top-5 classification accuracy,
				mean negative log-likelihood (NLL), and mean probability assigned to the
				correct class. These metrics are computed across the entire test set and
				optionally broken down by class for per-class analysis.
			</p>

		</div>
		<div class="margin-right-block"></div>
	</div>

	<div class="content-margin-container" id="results">
		<div class="margin-left-block"></div>
		<div class="main-content-block">
			<h1>Results</h1>

			<h2>Camouflage Task</h2>
			<p>
				Table 1 shows classification accuracy results on the COD10K test set across
				four experimental conditions: CLIP baseline, BASE SD + CLIP, RL-trained SD + CLIP,
				and SFT-trained SD + CLIP.
			</p>

			<table style="border-collapse: collapse; width: 100%; margin: 20px 0;">
				<thead>
					<tr style="border-bottom: 2px solid #333;">
						<th style="padding: 10px; text-align: left;">Method</th>
						<th style="padding: 10px; text-align: center;">Top-1 Accuracy</th>
						<th style="padding: 10px; text-align: center;">Top-3 Accuracy</th>
						<th style="padding: 10px; text-align: center;">Top-5 Accuracy</th>
					</tr>
				</thead>
				<tbody>
					<tr style="border-bottom: 1px solid #ddd;">
						<td style="padding: 10px;"><strong>CLIP Baseline</strong></td>
						<td style="padding: 10px; text-align: center;">44.87%</td>
						<td style="padding: 10px; text-align: center;">69.05%</td>
						<td style="padding: 10px; text-align: center;">77.20%</td>
					</tr>
					<tr style="border-bottom: 1px solid #ddd;">
						<td style="padding: 10px;"><strong>BASE SD + CLIP</strong></td>
						<td style="padding: 10px; text-align: center;">11.40%</td>
						<td style="padding: 10px; text-align: center;">26.60%</td>
						<td style="padding: 10px; text-align: center;">35.98%</td>
					</tr>
					<tr style="border-bottom: 1px solid #ddd;">
						<td style="padding: 10px;"><strong>RL-Trained SD + CLIP</strong></td>
						<td style="padding: 10px; text-align: center;">XX.X%</td>
						<td style="padding: 10px; text-align: center;">XX.X%</td>
						<td style="padding: 10px; text-align: center;">XX.X%</td>
					</tr>
					<tr>
						<td style="padding: 10px;"><strong>SFT-Trained SD + CLIP</strong></td>
						<td style="padding: 10px; text-align: center;">25.32%</td>
						<td style="padding: 10px; text-align: center;">45.95%</td>
						<td style="padding: 10px; text-align: center;">56.07%</td>
					</tr>
				</tbody>
			</table>

			<p>
				[Add brief analysis here: e.g., "Both RL and SFT training improve classification
				accuracy over the CLIP baseline, with SFT achieving XX.X% top-1 accuracy compared
				to XX.X% for CLIP alone. The BASE SD baseline shows that even untrained diffusion
				processing provides some benefit, suggesting the denoising process has an inherent
				regularizing effect."]
			</p>


			<h4>SFT-Trained Model</h4>
			<p>
				This model was trained for 30 epochs using the supervised fine-tuning method described above.
			</p>
			<div style="display: flex; flex-wrap: wrap; gap: 15px; margin: 20px 0; justify-content: center;">
				<!-- Pair 1 -->
				<div style="flex: 0 0 auto; width: 280px;">
					<div style="display: flex; gap: 8px;">
						<div style="flex: 1;">
							<img src="./images/good_no_prompt_sft_before_1.png" alt="Before: Original camouflaged image"
								style="width: 100%; border: 1px solid #ddd;" />
							<p class="image-caption-small">
								<strong>Before</strong><br />
								Grasshopper
							</p>
						</div>
						<div style="flex: 1;">
							<img src="./images/good_no_prompt_sft_after_1.png" alt="After: Enhanced image"
								style="width: 100%; border: 1px solid #ddd;" />
							<p class="image-caption-small">
								<strong>After</strong><br />
								Grasshopper
							</p>
						</div>
					</div>
				</div>

				<!-- Pair 2 -->
				<div style="flex: 0 0 auto; width: 280px;">
					<div style="display: flex; gap: 8px;">
						<div style="flex: 1;">
							<img src="./images/original_2.png" alt="Before: Original camouflaged image"
								style="width: 100%; border: 1px solid #ddd;" />
							<p class="image-caption-small">
								<strong>Before</strong><br />
								Frog
							</p>
						</div>
						<div style="flex: 1;">
							<img src="./images/no_prompt_sft_2.png" alt="After: Enhanced image"
								style="width: 100%; border: 1px solid #ddd;" />
							<p class="image-caption-small">
								<strong>After</strong><br />
								Frog
							</p>
						</div>
					</div>
				</div>

				<!-- Pair 3 (template) -->
				<div style="flex: 0 0 auto; width: 280px;">
					<div style="display: flex; gap: 8px;">
						<div style="flex: 1;">
							<img src="./images/original_3.png" alt="Before: Original camouflaged image"
								style="width: 100%; border: 1px solid #ddd;" />
							<p class="image-caption-small">
								<strong>Before</strong><br />
								Spider
							</p>
						</div>
						<div style="flex: 1;">
							<img src="./images/no_prompt_sft_3.png" alt="After: Enhanced image"
								style="width: 100%; border: 1px solid #ddd;" />
							<p class="image-caption-small">
								<strong>After</strong><br />
								Spider
							</p>
						</div>
					</div>
				</div>
			</div>

			<div style="display: flex; flex-wrap: wrap; gap: 15px; margin: 20px 0; justify-content: center;">
				<!-- Failure Pair 1 -->
				<div style="flex: 0 0 auto; width: 280px;">
					<div style="display: flex; gap: 8px;">
						<div style="flex: 1;">
							<img src="./images/fail_no_prompt_sft_before_4.png" alt="Before: Original camouflaged image"
								style="width: 100%; border: 1px solid #ddd;" />
							<p class="image-caption-small">
								<strong>Before</strong><br />
								Turtle<br />
							</p>
						</div>
						<div style="flex: 1;">
							<img src="./images/fail_no_prompt_sft_after_4.png" alt="After: Enhanced image"
								style="width: 100%; border: 1px solid #ddd;" />
							<p class="image-caption-small">
								<strong>After</strong><br />
								Owl<br />
							</p>
						</div>
					</div>
				</div>

				<!-- Failure Pair 2 -->
				<div style="flex: 0 0 auto; width: 280px;">
					<div style="display: flex; gap: 8px;">
						<div style="flex: 1;">
							<img src="./images/fail_no_prompt_sft_before_5.png" alt="Before: Original camouflaged image"
								style="width: 100%; border: 1px solid #ddd;" />
							<p class="image-caption-small">
								<strong>Before</strong><br />
								Snake<br />
							</p>
						</div>
						<div style="flex: 1;">
							<img src="./images/fail_no_prompt_sft_after_5.png" alt="After: Enhanced image"
								style="width: 100%; border: 1px solid #ddd;" />
							<p class="image-caption-small">
								<strong>After</strong><br />
								Frog<br />
							</p>
						</div>
					</div>
				</div>

				<!-- Failure Pair 3 -->
				<div style="flex: 0 0 auto; width: 280px;">
					<div style="display: flex; gap: 8px;">
						<div style="flex: 1;">
							<img src="./images/fail_no_prompt_sft_before_6.png" alt="Before: Original camouflaged image"
								style="width: 100%; border: 1px solid #ddd;" />
							<p class="image-caption-small">
								<strong>Before</strong><br />
								Spider<br />
							</p>
						</div>
						<div style="flex: 1;">
							<img src="./images/fail_no_prompt_sft_after_6.png" alt="After: Enhanced image"
								style="width: 100%; border: 1px solid #ddd;" />
							<p class="image-caption-small">
								<strong>After</strong><br />
								Moth<br />
							</p>
						</div>
					</div>
				</div>
			</div>

			<p>
				The first row shows results of "good" generations from diffusion model, while the second row shows poor
				generations that lead to misclassifications.
				The good generations show some promising behavior, such as a blurring of the background which enhances
				the camouflaged object.
				This is apparent in the frog and spider image pairs, where the background is noticeably blurred relative
				to the camouflaged object.
				The grasshopper's background is initialy blurred, but the diffusion model seems to do additionally blend
				and simplify the background colors.
			</p>
			<p>
				In each image pair of the "poor" generations, the diffusion model seems to misidentify the
				camouflaged object as a different class and denoise towards that data distribution. This explains why
				the SFT-trained model performs worse than the baseline
				camouflage dataset images. This is very noticeable in the snake-frog image pair, where the snake
				totally disappears and some resemblance of a frog appears. The spider-moth image pair shows another
				interesting behavior, where a "moth" object is seemingly
				generated independently. It is worth nothing that even to the human eye, the camouflage images for the
				"poor" generations are considerably harder, although
				they all belong to the same COD10K dataset without difficulty distinctions.
			</p>

			<div style="display: flex; flex-wrap: wrap; gap: 20px; margin: 20px 0; justify-content: center;">
				<div style="flex: 1; min-width: 400px; max-width: 500px;">
					<img src="./images/val_loss_comparison.png"
						alt="Validation loss comparison across prompt strategies"
						style="width: 100%; border: 1px solid #ddd;" />
					<p class="image-caption">
						Validation loss comparison
					</p>
				</div>
				<div style="flex: 1; min-width: 400px; max-width: 500px;">
					<img src="./images/val_accuracy_comparison.png"
						alt="Validation accuracy comparison across prompt strategies"
						style="width: 100%; border: 1px solid #ddd;" />
					<p class="image-caption">
						Validation accuracy comparison
					</p>
				</div>
			</div>

			<p>
				We also compared performance across different prompt
				strategies. Figure 2 shows validation loss and accuracy curves for models trained with
				custom prompts ("De-camouflage the animal", "Increase edge sharpness and contrast",
				"Enhance visibility of camouflaged animal") versus the no-prompt baseline. Despite
				the semantic differences in prompt wording, all custom prompts performed similarly to
				each other and showed no meaningful improvement over the no-prompt condition.		
			</p>

			
			<div style="display: flex; flex-wrap: wrap; gap: 15px; margin: 20px 0; justify-content: center;">
				<!-- Pair 1 -->
				<div style="flex: 0 0 auto; width: 280px;">
					<div style="display: flex; gap: 8px;">
						<div style="flex: 1;">
							<img src="./images/oracle_prompt_before_1.png" alt="Before: Original camouflaged image"
								style="width: 100%; border: 1px solid #ddd;" />
							<p class="image-caption-small">
								<strong>Before</strong><br />
								Cat
							</p>
						</div>
						<div style="flex: 1;">
							<img src="./images/oracle_prompt_after_1.png" alt="After: Enhanced image"
								style="width: 100%; border: 1px solid #ddd;" />
							<p class="image-caption-small">
								<strong>After</strong><br />
								Cat
							</p>
						</div>
					</div>
				</div>
				
				<!-- Pair 2 -->
				<div style="flex: 0 0 auto; width: 280px;">
					<div style="display: flex; gap: 8px;">
						<div style="flex: 1;">
							<img src="./images/oracle_prompt_before_2.png" alt="Before: Original camouflaged image"
								style="width: 100%; border: 1px solid #ddd;" />
							<p class="image-caption-small">
								<strong>Before</strong><br />
								Frog
							</p>
						</div>
						<div style="flex: 1;">
							<img src="./images/oracle_prompt_after_2.png" alt="After: Enhanced image"
								style="width: 100%; border: 1px solid #ddd;" />
							<p class="image-caption-small">
								<strong>After</strong><br />
								Frog
							</p>
						</div>
					</div>
				</div>
				
				<!-- Pair 3 -->
				<div style="flex: 0 0 auto; width: 280px;">
					<div style="display: flex; gap: 8px;">
						<div style="flex: 1;">
							<img src="./images/oracle_prompt_before_3.png" alt="Before: Original camouflaged image"
								style="width: 100%; border: 1px solid #ddd;" />
							<p class="image-caption-small">
								<strong>Before</strong><br />
								Mantis
							</p>
						</div>
						<div style="flex: 1;">
							<img src="./images/oracle_prompt_after_3.png" alt="After: Enhanced image"
								style="width: 100%; border: 1px solid #ddd;" />
							<p class="image-caption-small">
								<strong>After</strong><br />
								Mantis
							</p>
						</div>
					</div>
				</div>
			</div>

			<p>
				As 
				expected, the ORACLE prompt performed the best, but the generated images clearly deviate from the original image 
				and simply generates the class image. 
			</p>

			<h4>RL-Trained Model Results</h4>
			hector fill in with your before and after images
			<div style="display: flex; flex-wrap: wrap; gap: 15px; margin: 20px 0; justify-content: center;">
				<!-- Pair 1 -->
				<div style="flex: 0 0 auto; width: 280px;">
					<div style="display: flex; gap: 8px;">
						<div style="flex: 1;">
							<img src="./images/good_no_prompt_sft_before_1.png" alt="Before: Original camouflaged image"
								style="width: 100%; border: 1px solid #ddd;" />
							<p class="image-caption-small">
								<strong>Before</strong><br />
								Grasshopper
							</p>
						</div>
						<div style="flex: 1;">
							<img src="./images/good_no_prompt_sft_after_1.png" alt="After: Enhanced image"
								style="width: 100%; border: 1px solid #ddd;" />
							<p class="image-caption-small">
								<strong>After</strong><br />
								Grasshopper
							</p>
						</div>
					</div>
				</div>

				<!-- Pair 2 -->
				<div style="flex: 0 0 auto; width: 280px;">
					<div style="display: flex; gap: 8px;">
						<div style="flex: 1;">
							<img src="./images/original_2.png" alt="Before: Original camouflaged image"
								style="width: 100%; border: 1px solid #ddd;" />
							<p class="image-caption-small">
								<strong>Before</strong><br />
								Frog
							</p>
						</div>
						<div style="flex: 1;">
							<img src="./images/no_prompt_sft_2.png" alt="After: Enhanced image"
								style="width: 100%; border: 1px solid #ddd;" />
							<p class="image-caption-small">
								<strong>After</strong><br />
								Frog
							</p>
						</div>
					</div>
				</div>

				<!-- Pair 3 (template) -->
				<div style="flex: 0 0 auto; width: 280px;">
					<div style="display: flex; gap: 8px;">
						<div style="flex: 1;">
							<img src="./images/original_3.png" alt="Before: Original camouflaged image"
								style="width: 100%; border: 1px solid #ddd;" />
							<p class="image-caption-small">
								<strong>Before</strong><br />
								Spider
							</p>
						</div>
						<div style="flex: 1;">
							<img src="./images/no_prompt_sft_3.png" alt="After: Enhanced image"
								style="width: 100%; border: 1px solid #ddd;" />
							<p class="image-caption-small">
								<strong>After</strong><br />
								Spider
							</p>
						</div>
					</div>
				</div>
			</div>

			<div style="display: flex; flex-wrap: wrap; gap: 15px; margin: 20px 0; justify-content: center;">
				<!-- Failure Pair 1 -->
				<div style="flex: 0 0 auto; width: 280px;">
					<div style="display: flex; gap: 8px;">
						<div style="flex: 1;">
							<img src="./images/fail_no_prompt_sft_before_4.png" alt="Before: Original camouflaged image"
								style="width: 100%; border: 1px solid #ddd;" />
							<p class="image-caption-small">
								<strong>Before</strong><br />
								Turtle<br />
							</p>
						</div>
						<div style="flex: 1;">
							<img src="./images/fail_no_prompt_sft_after_4.png" alt="After: Enhanced image"
								style="width: 100%; border: 1px solid #ddd;" />
							<p class="image-caption-small">
								<strong>After</strong><br />
								Owl<br />
							</p>
						</div>
					</div>
				</div>

				<!-- Failure Pair 2 -->
				<div style="flex: 0 0 auto; width: 280px;">
					<div style="display: flex; gap: 8px;">
						<div style="flex: 1;">
							<img src="./images/fail_no_prompt_sft_before_5.png" alt="Before: Original camouflaged image"
								style="width: 100%; border: 1px solid #ddd;" />
							<p class="image-caption-small">
								<strong>Before</strong><br />
								Snake<br />
							</p>
						</div>
						<div style="flex: 1;">
							<img src="./images/fail_no_prompt_sft_after_5.png" alt="After: Enhanced image"
								style="width: 100%; border: 1px solid #ddd;" />
							<p class="image-caption-small">
								<strong>After</strong><br />
								Frog<br />
							</p>
						</div>
					</div>
				</div>

				<!-- Failure Pair 3 -->
				<div style="flex: 0 0 auto; width: 280px;">
					<div style="display: flex; gap: 8px;">
						<div style="flex: 1;">
							<img src="./images/fail_no_prompt_sft_before_6.png" alt="Before: Original camouflaged image"
								style="width: 100%; border: 1px solid #ddd;" />
							<p class="image-caption-small">
								<strong>Before</strong><br />
								Spider<br />
							</p>
						</div>
						<div style="flex: 1;">
							<img src="./images/fail_no_prompt_sft_after_6.png" alt="After: Enhanced image"
								style="width: 100%; border: 1px solid #ddd;" />
							<p class="image-caption-small">
								<strong>After</strong><br />
								Moth<br />
							</p>
						</div>
					</div>
				</div>
			</div>


			<p style="font-size: 0.9em; color: #666; font-style: italic; margin-top: 10px;">
				[Optional: Add more example pairs or a note about what visual changes the diffusion
				model makes, e.g., "The diffusion model enhances edge features and increases contrast
				in regions where the camouflaged object blends with the background."]
			</p>


			<h2>CIFAR-C Task (Luc or Hector - whoever trains)</h2>
			Comparing clip vs. clip+diffusion RL for CIFAR-C
		</div>
		<div class="margin-right-block">
		</div>
	</div>

	<div class="content-margin-container" id="discussion">
		<div class="margin-left-block"></div>
		<div class="main-content-block">
			<h1>Discussion</h1>
			<p>
				Our SFT-trained model achieves 25.32% top-1 accuracy, which falls short of the
				44.87% CLIP baseline but more than doubles the 11.40% accuracy of the untrained
				BASE SD model.
			</p>
			<p>
				When the model succeeds, it exhibits useful visual transformations like selectively
				blurring backgrounds while preserving foreground objects, which helps camouflaged
				subjects stand out. This suggests the model learns a meaningful "de-camouflage"
				operation in some cases. However, the failure cases reveal a clear pattern: the
				model misidentifies camouflaged objects and denoises toward the wrong class
				distribution, effectively hallucinating incorrect objects. This is particularly
				problematic for harder camouflage images where even humans struggle to identify the
				subject. Without explicit text guidance (empty prompts), the model lacks the semantic
				information needed to preserve the correct object, explaining the overall accuracy gap.
			</p>
			<p>
				The primary limitation is the accuracy-corruption tradeoff inherent in the I2I
				diffusion process. While capable of enhancing images, it can also corrupt them when
				the model misidentifies the subject. Additionally, using a diffusion model adds
				substantial inference latency (10-20 denoising steps), and our experiments are
				limited to the COD10K dataset (~6,000 images). The LoRA adapters are trained
				specifically for camouflage detection, so generalization to other domains (medical
				imaging, satellite imagery) would require separate fine-tuning on task-specific
				datasets. Several modifications could improve results: reducing noise strength from 0.4 to 0.2-0.3 to
				preserve more original structure;
				confidence-based gating to only enhance low-confidence images; or curriculum learning
				to train on easier samples first.
			</p>
			<p>
				While our SFT approach does not yet outperform the CLIP baseline, it demonstrates
				that diffusion models can learn meaningful image transformations for classification
				tasks. The significant improvement over BASE SD and the promising behavior in
				successful cases suggest that with better guidance mechanisms, diffusion-based
				enhancement remains a viable approach for challenging perception tasks like
				camouflage detection.
			</p>
		</div>
		<div class="margin-right-block"></div>
	</div>

	<div class="content-margin-container" id="citations">
		<div class="margin-left-block"></div>
		<div class="main-content-block">
			<div class="citation" id="references" style="height: auto">
				<br />
				<span style="font-size: 16px">References:</span><br /><br />
				<ol class="references">
					<li id="ref_ho">
						Ho, J., Jain, A., & Abbeel, P. (2020).
						<a href="https://arxiv.org/abs/2006.11239">Denoising Diffusion Probabilistic Models</a>.
						NeurIPS 2020.
					</li>
					<li id="ref_sohldickstein">
						Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., & Ganguli, S. (2015).
						<a href="https://arxiv.org/abs/1503.03585">Deep Unsupervised Learning using Nonequilibrium Thermodynamics</a>.
						ICML 2015.
					</li>
					<li id="ref_ho_cfg">
						Ho, J., & Salimans, T. (2022).
						<a href="https://arxiv.org/abs/2207.12598">Classifier-Free Diffusion Guidance</a>.
						arXiv:2207.12598.
					</li>
					<li id="ref_brooks">
						Brooks, T., et al. (2023).
						<a href="https://arxiv.org/abs/2211.09800">InstructPix2Pix: Learning to Follow Image Editing Instructions</a>.
						arXiv:2211.09800.
					</li>
					<li id="ref_zhang">
						Zhang, L., et al. (2023).
						<a href="https://arxiv.org/abs/2302.05543">Adding Conditional Control to Text-to-Image Diffusion Models</a>.
						arXiv:2302.05543.
					</li>
					<li id="ref_rombach">
						Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2022).
						<a href="https://arxiv.org/abs/2112.10752">High-Resolution Image Synthesis with Latent Diffusion Models</a>.
						CVPR 2022.
					</li>
					<li id="ref_song">
						Song, J., Meng, C., & Ermon, S. (2021).
						<a href="https://arxiv.org/abs/2010.02502">Denoising Diffusion Implicit Models</a>.
						ICLR 2021.
					</li>
					<li id="ref_radford">
						Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., ... & Sutskever, I. (2021).
						<a href="https://arxiv.org/abs/2103.00020">Learning Transferable Visual Models From Natural Language Supervision</a>.
						ICML 2021.
					</li>
					<li id="ref_black">
						Black, K., Janner, M., Du, Y., Kostrikov, I., & Levine, S. (2023).
						<a href="https://arxiv.org/abs/2305.13301">Training Diffusion Models with Reinforcement Learning</a>.
						NeurIPS 2023.
					</li>
					<li id="ref_fan">
						Fan, D.-P., et al. (2021).
						<a href="https://doi.org/10.48550/arXiv.2102.10274">Concealed Object Detection</a>.
						arXiv:2102.10274.
					</li>
					<li id="ref_hendrycks">
						Hendrycks, D., & Dietterich, T. (2019).
						<a href="https://doi.org/10.48550/arXiv.1903.12261">Benchmarking Neural Network Robustness to Common Corruptions and Perturbations</a>.
						arXiv:1903.12261.
					</li>
				</ol>
			</div>
		</div>
		<div class="margin-right-block"></div>
	</div>
	<script>
		document.addEventListener("DOMContentLoaded", function () {
			const refList = document.querySelector('ol.references');
			if (!refList) return;

			// 1. Gather reference data
			const refData = {};
			const refUrls = {};
			Array.from(refList.children).forEach(item => {
				if (item.id) {
					refData[item.id] = item.innerHTML;
					const link = item.querySelector('a');
					if (link) {
						refUrls[item.id] = link.getAttribute('href');
					}
				}
			});

			// 2. Find all citation links in the document order
			const citations = document.querySelectorAll('a.cite');
			let citationCount = 0;
			const citedIDs = new Map(); // id -> number
			const marginDisplayedIDs = new Set(); // Track IDs already shown in margin

			citations.forEach(cite => {
				const href = cite.getAttribute('href');
				if (href && href.startsWith('#')) {
					const id = href.substring(1);
					
					// Assign number if first time seen
					if (!citedIDs.has(id)) {
						citationCount++;
						citedIDs.set(id, citationCount);
					}
					
					const num = citedIDs.get(id);
					cite.textContent = `[${num}]`;
					
					// Update link to point to external URL
					if (refUrls[id]) {
						cite.setAttribute('href', refUrls[id]);
						cite.setAttribute('target', '_blank');
					}

					// Create margin note if not already displayed
					if (!marginDisplayedIDs.has(id)) {
						marginDisplayedIDs.add(id);
						
						const container = cite.closest('.content-margin-container');
						if (container) {
							const rightMargin = container.querySelector('.margin-right-block');
							if (rightMargin) {
								const note = document.createElement('div');
								note.className = 'citation-margin';
								note.innerHTML = `<span style="font-weight: bold;">${num}</span> ${refData[id]}`;
								
								rightMargin.appendChild(note);
							}
						}
					}
				}
			});
		});
	</script>
</body>

</html>